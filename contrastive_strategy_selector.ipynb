{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "%pip install -q sentence-transformers torch scikit-learn numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/blank/Documents/Foundation Models Course Projects/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Simplification types: ['sentence_splitting', 'vocabulary_simplification', 'structure_reordering', 'deletion', 'definition_or_expansion']\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import pickle\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Fixed simplification types\n",
        "LABELS = [\n",
        "    \"sentence_splitting\",\n",
        "    \"vocabulary_simplification\",\n",
        "    \"structure_reordering\",\n",
        "    \"deletion\",\n",
        "    \"definition_or_expansion\"\n",
        "]\n",
        "\n",
        "print(f\"Simplification types: {LABELS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total examples: 2005\n",
            "\n",
            "Simplification type distribution:\n",
            "  sentence_splitting: 123 (6.1%)\n",
            "  vocabulary_simplification: 1453 (72.5%)\n",
            "  structure_reordering: 257 (12.8%)\n",
            "  deletion: 131 (6.5%)\n",
            "  definition_or_expansion: 41 (2.0%)\n",
            "\n",
            "Sample example:\n",
            "  Sentence: 1. በዚህ አንቀጽ ንዑስ አንቀጽ መሰረት የሚቀርበው ክስ ገንዘብ ጠያቂው የማህበሩን መፍረስ ካወቀበት ጊዜ ጀምሮ በአምስት ዓመት ውስጥ ካልቀረበ በይርጋ ይታገዳ...\n",
            "  Type: vocabulary_simplification\n"
          ]
        }
      ],
      "source": [
        "# Load dataset from regen.json\n",
        "DATA_PATH = \"Dataset/final_dataset/regen.json\"\n",
        "\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract (sentence, simplification_type) pairs\n",
        "examples = []\n",
        "for item in data:\n",
        "    if \"legal_sentence\" in item and \"simplification_type\" in item:\n",
        "        examples.append((item[\"legal_sentence\"], item[\"simplification_type\"]))\n",
        "\n",
        "print(f\"Total examples: {len(examples)}\")\n",
        "\n",
        "# Check distribution of simplification types\n",
        "type_counts = defaultdict(int)\n",
        "for _, label in examples:\n",
        "    type_counts[label] += 1\n",
        "\n",
        "print(\"\\nSimplification type distribution:\")\n",
        "for label in LABELS:\n",
        "    count = type_counts.get(label, 0)\n",
        "    print(f\"  {label}: {count} ({count/len(examples)*100:.1f}%)\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample example:\")\n",
        "print(f\"  Sentence: {examples[0][0][:100]}...\")\n",
        "print(f\"  Type: {examples[0][1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Sentence Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder loaded: paraphrase-multilingual-MiniLM-L12-v2\n",
            "Encoder device: mps\n",
            "Embedding dimension: 384\n"
          ]
        }
      ],
      "source": [
        "# Load multilingual sentence encoder\n",
        "# This model is for strategy selection, not generation\n",
        "encoder = SentenceTransformer(\n",
        "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "print(f\"Encoder loaded: paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "print(f\"Encoder device: {device}\")\n",
        "\n",
        "# Test encoding\n",
        "test_emb = encoder.encode([\"Test sentence\"], convert_to_numpy=True)\n",
        "print(f\"Embedding dimension: {test_emb.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build Contrastive Training Pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building pairs from 5 labels...\n",
            "\n",
            "Total contrastive pairs: 8000\n",
            "Positive pairs: 4000\n",
            "Negative pairs: 4000\n"
          ]
        }
      ],
      "source": [
        "def build_pairs(examples, max_pairs=8000):\n",
        "    \"\"\"\n",
        "    Build positive and negative pairs for contrastive learning.\n",
        "    - Positive pair: same simplification_type\n",
        "    - Negative pair: different simplification_type\n",
        "    \"\"\"\n",
        "    by_label = defaultdict(list)\n",
        "    for sent, label in examples:\n",
        "        by_label[label].append(sent)\n",
        "    \n",
        "    pairs = []\n",
        "    labels = list(by_label.keys())\n",
        "    \n",
        "    # Ensure we have at least 2 examples per label for positive pairs\n",
        "    valid_labels = [l for l in labels if len(by_label[l]) >= 2]\n",
        "    \n",
        "    if not valid_labels:\n",
        "        raise ValueError(\"Need at least 2 examples per label for contrastive learning\")\n",
        "    \n",
        "    print(f\"Building pairs from {len(valid_labels)} labels...\")\n",
        "    \n",
        "    while len(pairs) < max_pairs:\n",
        "        # Positive pair\n",
        "        label = random.choice(valid_labels)\n",
        "        if len(by_label[label]) >= 2:\n",
        "            s1, s2 = random.sample(by_label[label], 2)\n",
        "            pairs.append((s1, s2, 1))\n",
        "        \n",
        "        # Negative pair\n",
        "        if len(valid_labels) > 1:\n",
        "            neg_label = random.choice([l for l in valid_labels if l != label])\n",
        "            if len(by_label[label]) > 0 and len(by_label[neg_label]) > 0:\n",
        "                s1 = random.choice(by_label[label])\n",
        "                s3 = random.choice(by_label[neg_label])\n",
        "                pairs.append((s1, s3, 0))\n",
        "    \n",
        "    return pairs[:max_pairs]\n",
        "\n",
        "# Build training pairs\n",
        "pairs = build_pairs(examples, max_pairs=8000)\n",
        "print(f\"\\nTotal contrastive pairs: {len(pairs)}\")\n",
        "print(f\"Positive pairs: {sum(1 for _, _, label in pairs if label == 1)}\")\n",
        "print(f\"Negative pairs: {sum(1 for _, _, label in pairs if label == 0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Contrastive Dataset and Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 8000\n",
            "Batches per epoch: 500\n"
          ]
        }
      ],
      "source": [
        "class ContrastiveDataset(Dataset):\n",
        "    \"\"\"Dataset for contrastive learning pairs\"\"\"\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]\n",
        "\n",
        "def contrastive_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to properly handle tuple batches.\n",
        "    Batch is a list of tuples: [(s1, s2, label), ...]\n",
        "    Returns: (s1_list, s2_list, labels_list)\n",
        "    \"\"\"\n",
        "    s1_list = [item[0] for item in batch]\n",
        "    s2_list = [item[1] for item in batch]\n",
        "    labels_list = [item[2] for item in batch]\n",
        "    return (s1_list, s2_list, labels_list)\n",
        "\n",
        "def contrastive_loss(e1, e2, label, margin=0.5):\n",
        "    \"\"\"\n",
        "    Contrastive loss with cosine similarity and margin.\n",
        "    - Positive pairs (label=1): minimize distance\n",
        "    - Negative pairs (label=0): maximize distance beyond margin\n",
        "    \"\"\"\n",
        "    cosine = F.cosine_similarity(e1, e2)\n",
        "    pos_loss = (1 - cosine) * label\n",
        "    neg_loss = torch.clamp(cosine - margin, min=0.0) * (1 - label)\n",
        "    return (pos_loss + neg_loss).mean()\n",
        "\n",
        "# Create dataset and dataloader with custom collate function\n",
        "dataset = ContrastiveDataset(pairs)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=contrastive_collate_fn)\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Batches per epoch: {len(loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking batch structure...\n",
            "Batch type: <class 'tuple'>\n",
            "Batch length: 3\n",
            "Batch structure: (s1_list, s2_list, labels_list)\n",
            "s1_list length: 2\n",
            "s2_list length: 2\n",
            "labels_list length: 2\n",
            "Labels: [1, 0]\n",
            "Label types: [<class 'int'>, <class 'int'>]\n"
          ]
        }
      ],
      "source": [
        "# Debug: Check batch structure (with custom collate function)\n",
        "print(\"Checking batch structure...\")\n",
        "test_loader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=contrastive_collate_fn)\n",
        "test_batch = next(iter(test_loader))\n",
        "print(f\"Batch type: {type(test_batch)}\")\n",
        "print(f\"Batch length: {len(test_batch)}\")\n",
        "print(f\"Batch structure: (s1_list, s2_list, labels_list)\")\n",
        "print(f\"s1_list length: {len(test_batch[0])}\")\n",
        "print(f\"s2_list length: {len(test_batch[1])}\")\n",
        "print(f\"labels_list length: {len(test_batch[2])}\")\n",
        "print(f\"Labels: {test_batch[2]}\")\n",
        "print(f\"Label types: {[type(l) for l in test_batch[2]]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Contrastive Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting contrastive training...\n",
            "Epochs: 3, Batch size: 16, Device: mps\n",
            "\n",
            "  Batch 50/500, Loss: 0.2734\n",
            "  Batch 100/500, Loss: 0.2473\n",
            "  Batch 150/500, Loss: 0.1986\n",
            "  Batch 200/500, Loss: 0.2624\n",
            "  Batch 250/500, Loss: 0.1853\n",
            "  Batch 300/500, Loss: 0.1816\n",
            "  Batch 350/500, Loss: 0.1489\n",
            "  Batch 400/500, Loss: 0.1053\n",
            "  Batch 450/500, Loss: 0.1186\n",
            "  Batch 500/500, Loss: 0.0984\n",
            "Epoch 1/3 | Average Loss: 0.2082\n",
            "\n",
            "  Batch 50/500, Loss: 0.1257\n",
            "  Batch 100/500, Loss: 0.1357\n",
            "  Batch 150/500, Loss: 0.0933\n",
            "  Batch 200/500, Loss: 0.1250\n",
            "  Batch 250/500, Loss: 0.0670\n",
            "  Batch 300/500, Loss: 0.0672\n",
            "  Batch 350/500, Loss: 0.1396\n",
            "  Batch 400/500, Loss: 0.1311\n",
            "  Batch 450/500, Loss: 0.0874\n",
            "  Batch 500/500, Loss: 0.1621\n",
            "Epoch 2/3 | Average Loss: 0.1276\n",
            "\n",
            "  Batch 50/500, Loss: 0.1121\n",
            "  Batch 100/500, Loss: 0.0916\n",
            "  Batch 150/500, Loss: 0.1848\n",
            "  Batch 200/500, Loss: 0.0625\n",
            "  Batch 250/500, Loss: 0.0833\n",
            "  Batch 300/500, Loss: 0.0717\n",
            "  Batch 350/500, Loss: 0.0955\n",
            "  Batch 400/500, Loss: 0.0948\n",
            "  Batch 450/500, Loss: 0.0695\n",
            "  Batch 500/500, Loss: 0.0763\n",
            "Epoch 3/3 | Average Loss: 0.0872\n",
            "\n",
            "Training complete!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
              "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set up optimizer\n",
        "optimizer = AdamW(encoder.parameters(), lr=2e-5)\n",
        "\n",
        "# Training loop\n",
        "encoder.train()\n",
        "EPOCHS = 3  # Light fine-tuning, enough for strategy separation\n",
        "\n",
        "print(\"Starting contrastive training...\")\n",
        "print(f\"Epochs: {EPOCHS}, Batch size: 16, Device: {device}\\n\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, batch in enumerate(loader):\n",
        "        # Batch is now (s1_list, s2_list, labels_list) thanks to custom collate_fn\n",
        "        s1_list, s2_list, labels_list = batch\n",
        "        \n",
        "        # Convert labels to tensor (should already be numeric: 1 or 0)\n",
        "        labels = torch.tensor([float(label) for label in labels_list], dtype=torch.float32).to(device)\n",
        "        \n",
        "        # Encode sentences using forward pass (for training with gradients)\n",
        "        # sentence-transformers' encode() uses no_grad, so we use the model's forward directly\n",
        "        # Get embeddings with gradients enabled\n",
        "        features1 = encoder.tokenize(s1_list)\n",
        "        features1 = {k: v.to(device) for k, v in features1.items()}\n",
        "        e1 = encoder(features1)['sentence_embedding']\n",
        "        \n",
        "        features2 = encoder.tokenize(s2_list)\n",
        "        features2 = {k: v.to(device) for k, v in features2.items()}\n",
        "        e2 = encoder(features2)['sentence_embedding']\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = contrastive_loss(e1, e2, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        # Progress update every 50 batches\n",
        "        if (batch_idx + 1) % 50 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{len(loader)}, Loss: {loss.item():.4f}\")\n",
        "    \n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Average Loss: {avg_loss:.4f}\\n\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "encoder.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Build Class Centroids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building centroids for each simplification type...\n",
            "  sentence_splitting: 123 examples, centroid shape: (384,)\n",
            "  vocabulary_simplification: 1453 examples, centroid shape: (384,)\n",
            "  structure_reordering: 257 examples, centroid shape: (384,)\n",
            "  deletion: 131 examples, centroid shape: (384,)\n",
            "  definition_or_expansion: 41 examples, centroid shape: (384,)\n",
            "\n",
            "Centroids built for 5 types\n"
          ]
        }
      ],
      "source": [
        "# Build centroids for each simplification type\n",
        "# Centroids represent the \"average\" embedding for each strategy type\n",
        "encoder.eval()\n",
        "\n",
        "centroids = {}\n",
        "print(\"Building centroids for each simplification type...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for label in LABELS:\n",
        "        # Get all sentences for this label\n",
        "        sents = [s for s, l in examples if l == label]\n",
        "        \n",
        "        if len(sents) == 0:\n",
        "            print(f\"  Warning: No examples found for {label}\")\n",
        "            continue\n",
        "        \n",
        "        # Encode all sentences for this label\n",
        "        emb = encoder.encode(sents, convert_to_numpy=True, show_progress_bar=False)\n",
        "        \n",
        "        # Compute centroid (mean embedding)\n",
        "        centroids[label] = emb.mean(axis=0)\n",
        "        print(f\"  {label}: {len(sents)} examples, centroid shape: {centroids[label].shape}\")\n",
        "\n",
        "print(f\"\\nCentroids built for {len(centroids)} types\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Model and Centroids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder saved to: models/contrastive_strategy_selector\n",
            "Centroids saved to: models/contrastive_strategy_selector/centroids.pkl\n",
            "Labels saved to: models/contrastive_strategy_selector/labels.txt\n",
            "\n",
            "✅ All files saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create save directory\n",
        "SAVE_DIR = \"models/contrastive_strategy_selector\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Save the fine-tuned encoder\n",
        "encoder.save(SAVE_DIR)\n",
        "print(f\"Encoder saved to: {SAVE_DIR}\")\n",
        "\n",
        "# Save centroids\n",
        "centroids_path = os.path.join(SAVE_DIR, \"centroids.pkl\")\n",
        "with open(centroids_path, \"wb\") as f:\n",
        "    pickle.dump(centroids, f)\n",
        "print(f\"Centroids saved to: {centroids_path}\")\n",
        "\n",
        "# Save labels for reference\n",
        "labels_path = os.path.join(SAVE_DIR, \"labels.txt\")\n",
        "with open(labels_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for label in LABELS:\n",
        "        f.write(f\"{label}\\n\")\n",
        "print(f\"Labels saved to: {labels_path}\")\n",
        "\n",
        "print(\"\\n✅ All files saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Load Saved Model (for future use)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nLOAD_DIR = \"models/contrastive_strategy_selector\"\\n\\n# Load encoder\\nloaded_encoder = SentenceTransformer(LOAD_DIR)\\nloaded_encoder = loaded_encoder.to(device)\\n\\n# Load centroids\\nwith open(os.path.join(LOAD_DIR, \"centroids.pkl\"), \"rb\") as f:\\n    loaded_centroids = pickle.load(f)\\n\\nprint(\"Model and centroids loaded successfully!\")\\n'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example: How to load the saved model and centroids\n",
        "# Uncomment and run this cell when you want to load a previously trained model\n",
        "\n",
        "\"\"\"\n",
        "LOAD_DIR = \"models/contrastive_strategy_selector\"\n",
        "\n",
        "# Load encoder\n",
        "loaded_encoder = SentenceTransformer(LOAD_DIR)\n",
        "loaded_encoder = loaded_encoder.to(device)\n",
        "\n",
        "# Load centroids\n",
        "with open(os.path.join(LOAD_DIR, \"centroids.pkl\"), \"rb\") as f:\n",
        "    loaded_centroids = pickle.load(f)\n",
        "\n",
        "print(\"Model and centroids loaded successfully!\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Hybrid Inference System (Heuristics + Contrastive)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hybrid inference system ready!\n"
          ]
        }
      ],
      "source": [
        "def heuristic_predict(sentence):\n",
        "    \"\"\"\n",
        "    Heuristic rules for high-confidence strategy selection.\n",
        "    Returns simplification_type if confident, None otherwise.\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    word_count = len(words)\n",
        "    \n",
        "    # Sentence splitting: very long sentences\n",
        "    if word_count > 40:\n",
        "        return \"sentence_splitting\"\n",
        "    \n",
        "    # Deletion: presence of known boilerplate phrases\n",
        "    boilerplate_phrases = [\n",
        "        \"እንደተጠበቀ ሆኖ\",\n",
        "        \"በማንኛውም ሁኔታ\",\n",
        "        \"ያለ አግባብ\",\n",
        "        \"እንደተጠበቀ\",\n",
        "    ]\n",
        "    if any(phrase in sentence for phrase in boilerplate_phrases):\n",
        "        return \"deletion\"\n",
        "    \n",
        "    # Structure reordering: multiple conjunctions/clauses\n",
        "    conjunctions = [\"እና\", \"ወይም\", \"ቢሆንም\", \"ነገር ግን\"]\n",
        "    conjunction_count = sum(1 for conj in conjunctions if conj in sentence)\n",
        "    if conjunction_count >= 3:\n",
        "        return \"structure_reordering\"\n",
        "    \n",
        "    # No high-confidence heuristic match\n",
        "    return None\n",
        "\n",
        "def contrastive_predict(sentence, encoder, centroids):\n",
        "    \"\"\"\n",
        "    Use contrastive embeddings to predict simplification type.\n",
        "    Returns the type with highest cosine similarity to its centroid.\n",
        "    \"\"\"\n",
        "    # Encode the sentence\n",
        "    emb = encoder.encode([sentence], convert_to_numpy=True)[0]\n",
        "    \n",
        "    # Compute similarity to each centroid\n",
        "    sims = {}\n",
        "    for label, centroid in centroids.items():\n",
        "        similarity = cosine_similarity([emb], [centroid])[0][0]\n",
        "        sims[label] = similarity\n",
        "    \n",
        "    # Return type with highest similarity\n",
        "    return max(sims, key=sims.get), sims\n",
        "\n",
        "def select_simplification_type(sentence, encoder, centroids):\n",
        "    \"\"\"\n",
        "    Hybrid selector: heuristics first, contrastive fallback.\n",
        "    Returns (simplification_type, method_used, confidence_scores)\n",
        "    \"\"\"\n",
        "    # Try heuristics first\n",
        "    heuristic_result = heuristic_predict(sentence)\n",
        "    if heuristic_result:\n",
        "        return heuristic_result, \"heuristic\", None\n",
        "    \n",
        "    # Fallback to contrastive\n",
        "    predicted_type, sims = contrastive_predict(sentence, encoder, centroids)\n",
        "    return predicted_type, \"contrastive\", sims\n",
        "\n",
        "print(\"Hybrid inference system ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing hybrid strategy selector:\n",
            "\n",
            "Example 1:\n",
            "  Sentence: 1. በዚህ አንቀጽ ንዑስ አንቀጽ መሰረት የሚቀርበው ክስ ገንዘብ ጠያቂው የማህበሩን መፍረስ ካወቀበት ጊዜ ጀምሮ በአምስት ዓመት...\n",
            "  True label: vocabulary_simplification\n",
            "  Predicted: structure_reordering (via contrastive)\n",
            "  Similarities: structure_reordering: 0.996, vocabulary_simplification: 0.946, deletion: 0.798, sentence_splitting: 0.407, definition_or_expansion: -0.189\n",
            "  Match: ❌\n",
            "\n",
            "Example 11:\n",
            "  Sentence: 11. ቦርድ አስፈላጊ መስል በታየው ጊዜ ሁሉ ከአባላቱ መካከል ጉዳዮችን መርምረው የመፍትሔ ሏሳብ የሚያቀርቡ ኮሚቴዎችን ማቋቋም...\n",
            "  True label: vocabulary_simplification\n",
            "  Predicted: vocabulary_simplification (via contrastive)\n",
            "  Similarities: vocabulary_simplification: 0.996, structure_reordering: 0.979, deletion: 0.926, sentence_splitting: 0.147, definition_or_expansion: -0.115\n",
            "  Match: ✅\n",
            "\n",
            "Example 51:\n",
            "  Sentence: 51. ጉባኤው ሥራ ከመጀመሩ በፊት ስብሰባው ላይ ከተገኙት ባለአክሲዮኖች መካከል በጉባኤው ላይ የተደረገው ውይይት እና የተላለፈ...\n",
            "  True label: sentence_splitting\n",
            "  Predicted: sentence_splitting (via contrastive)\n",
            "  Similarities: sentence_splitting: 0.996, structure_reordering: 0.260, vocabulary_simplification: 0.026, deletion: -0.265, definition_or_expansion: -0.271\n",
            "  Match: ✅\n",
            "\n",
            "Example 101:\n",
            "  Sentence: 101. ለገንዘብ ጠያቂዎች ተመጣጣኝ ዋስትና ካልተገባላቸው ወይም ከማህበሩ የሚጠየቁ ግዴታዎች እስከሚፈጸሙ ድረስ የሀብት ክፍፍለ...\n",
            "  True label: structure_reordering\n",
            "  Predicted: structure_reordering (via contrastive)\n",
            "  Similarities: structure_reordering: 0.900, vocabulary_simplification: 0.764, sentence_splitting: 0.708, deletion: 0.526, definition_or_expansion: -0.274\n",
            "  Match: ✅\n",
            "\n",
            "Example 201:\n",
            "  Sentence: 201. ይህንንም የያዘው ሦስተኛ ወገን የሆነው ሰው በዋስትና መያዣነት ገንዘብ ጠያቂዎች ለሆኑት የሚፈለጉትን ገንዘብ በሙሉ ካል...\n",
            "  True label: structure_reordering\n",
            "  Predicted: structure_reordering (via contrastive)\n",
            "  Similarities: structure_reordering: 0.993, vocabulary_simplification: 0.932, deletion: 0.772, sentence_splitting: 0.435, definition_or_expansion: -0.214\n",
            "  Match: ✅\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test on a few examples\n",
        "print(\"Testing hybrid strategy selector:\\n\")\n",
        "\n",
        "test_indices = [0, 10, 50, 100, 200]  # Sample different examples\n",
        "\n",
        "for idx in test_indices:\n",
        "    if idx < len(examples):\n",
        "        sentence, true_label = examples[idx]\n",
        "        \n",
        "        # Get prediction\n",
        "        pred_type, method, sims = select_simplification_type(sentence, encoder, centroids)\n",
        "        \n",
        "        # Display results\n",
        "        print(f\"Example {idx + 1}:\")\n",
        "        print(f\"  Sentence: {sentence[:80]}...\")\n",
        "        print(f\"  True label: {true_label}\")\n",
        "        print(f\"  Predicted: {pred_type} (via {method})\")\n",
        "        if sims:\n",
        "            print(f\"  Similarities: {', '.join([f'{k}: {v:.3f}' for k, v in sorted(sims.items(), key=lambda x: x[1], reverse=True)])}\")\n",
        "        print(f\"  Match: {'✅' if pred_type == true_label else '❌'}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Evaluation: Accuracy by Method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating hybrid system on full dataset...\n",
            "\n",
            "Overall Results:\n",
            "  Total examples: 2005\n",
            "  Correct predictions: 1039 (51.82%)\n",
            "\n",
            "By Method:\n",
            "  Heuristic:\n",
            "    Used: 158 (7.9%)\n",
            "    Accuracy: 36.08%\n",
            "  Contrastive:\n",
            "    Used: 1847 (92.1%)\n",
            "    Accuracy: 53.17%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on all examples\n",
        "print(\"Evaluating hybrid system on full dataset...\\n\")\n",
        "\n",
        "correct = 0\n",
        "heuristic_count = 0\n",
        "contrastive_count = 0\n",
        "heuristic_correct = 0\n",
        "contrastive_correct = 0\n",
        "\n",
        "for sentence, true_label in examples:\n",
        "    pred_type, method, _ = select_simplification_type(sentence, encoder, centroids)\n",
        "    \n",
        "    if pred_type == true_label:\n",
        "        correct += 1\n",
        "    \n",
        "    if method == \"heuristic\":\n",
        "        heuristic_count += 1\n",
        "        if pred_type == true_label:\n",
        "            heuristic_correct += 1\n",
        "    else:\n",
        "        contrastive_count += 1\n",
        "        if pred_type == true_label:\n",
        "            contrastive_correct += 1\n",
        "\n",
        "total = len(examples)\n",
        "accuracy = correct / total * 100\n",
        "heuristic_accuracy = (heuristic_correct / heuristic_count * 100) if heuristic_count > 0 else 0\n",
        "contrastive_accuracy = (contrastive_correct / contrastive_count * 100) if contrastive_count > 0 else 0\n",
        "\n",
        "print(f\"Overall Results:\")\n",
        "print(f\"  Total examples: {total}\")\n",
        "print(f\"  Correct predictions: {correct} ({accuracy:.2f}%)\")\n",
        "print(f\"\\nBy Method:\")\n",
        "print(f\"  Heuristic:\")\n",
        "print(f\"    Used: {heuristic_count} ({heuristic_count/total*100:.1f}%)\")\n",
        "print(f\"    Accuracy: {heuristic_accuracy:.2f}%\")\n",
        "print(f\"  Contrastive:\")\n",
        "print(f\"    Used: {contrastive_count} ({contrastive_count/total*100:.1f}%)\")\n",
        "print(f\"    Accuracy: {contrastive_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Integration Function for AfriByT5\n",
        "\n",
        "This function can be imported and used in your AfriByT5 inference pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Integration function ready!\n",
            "\n",
            "To use in your AfriByT5 pipeline:\n",
            "  from contrastive_strategy_selector import get_strategy_selector\n",
            "  selector = get_strategy_selector()\n",
            "  simplification_type = selector(legal_sentence)\n",
            "  prompt = f'simplify | {simplification_type}: {legal_sentence}'\n"
          ]
        }
      ],
      "source": [
        "def get_strategy_selector(model_dir=\"models/contrastive_strategy_selector\", device=None):\n",
        "    \"\"\"\n",
        "    Load the trained strategy selector (encoder + centroids).\n",
        "    Returns a function that can be used to select simplification_type.\n",
        "    \n",
        "    Usage:\n",
        "        selector = get_strategy_selector()\n",
        "        simplification_type = selector(\"Your legal sentence here\")\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "    \n",
        "    # Load encoder\n",
        "    encoder = SentenceTransformer(model_dir)\n",
        "    encoder = encoder.to(device)\n",
        "    \n",
        "    # Load centroids\n",
        "    centroids_path = os.path.join(model_dir, \"centroids.pkl\")\n",
        "    with open(centroids_path, \"rb\") as f:\n",
        "        centroids = pickle.load(f)\n",
        "    \n",
        "    # Return a callable function\n",
        "    def select_type(sentence):\n",
        "        pred_type, method, _ = select_simplification_type(sentence, encoder, centroids)\n",
        "        return pred_type\n",
        "    \n",
        "    return select_type\n",
        "\n",
        "# Example usage:\n",
        "# selector = get_strategy_selector()\n",
        "# simplification_type = selector(\"Your legal sentence\")\n",
        "# Then use this type to condition AfriByT5: f\"simplify | {simplification_type}: {sentence}\"\n",
        "\n",
        "print(\"Integration function ready!\")\n",
        "print(\"\\nTo use in your AfriByT5 pipeline:\")\n",
        "print(\"  from contrastive_strategy_selector import get_strategy_selector\")\n",
        "print(\"  selector = get_strategy_selector()\")\n",
        "print(\"  simplification_type = selector(legal_sentence)\")\n",
        "print(\"  prompt = f'simplify | {simplification_type}: {legal_sentence}'\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
