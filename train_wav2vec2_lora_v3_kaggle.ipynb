{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoZMmaS-RHVA"
   },
   "source": [
    "# Ablation V3: LoRA Lower Capacity (r=4, alpha=8)\n",
    "\n",
    "This notebook is part of an ablation study to compare different fine-tuning approaches.\n",
    "\n",
    "## Model and Approach\n",
    "- **Base Model**: `agkphysics/wav2vec2-large-xlsr-53-amharic`\n",
    "- **Fine-tuning Method**: LoRA (r=4, alpha=8)\n",
    "- **Task**: Automatic Speech Recognition (ASR)\n",
    "- **Domain**: Legal Amharic text\n",
    "- **Dataset**: Dataset_4.0h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DBs0OtVRHVN"
   },
   "source": [
    "## 1. Installation and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force single GPU usage to avoid DataParallel issues on Kaggle\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oA0TIzXYRHVO",
    "outputId": "0233ec43-7a25-4f59-b5a5-81c43716b54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets accelerate peft torchaudio librosa jiwer soundfile matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiAJxozFRHVQ",
    "outputId": "1d7a9bc9-1af4-4baf-9596-a3fec00bc10e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "CUDA device: Tesla T4\n",
      "CUDA memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import jiwer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seed_train_wav2vec2_lora_v3"
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42  # Change to 123, 456 for different seeds\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Random seed set to: {SEED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rfSjXrKbSAY3",
    "outputId": "7f7f577c-8523-448c-e4a2-0f6925bd02dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Kaggle: Dataset is uploaded as input dataset\n",
    "# No need to mount Google Drive\n",
    "print(\"Running on Kaggle - using input dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtdlWzztRHVS"
   },
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HRLhSmZRHVT",
    "outputId": "c72e267d-0ea5-410f-9bea-b3beb7f97be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: agkphysics/wav2vec2-large-xlsr-53-amharic\n",
      "  Output directory: wav2vec2_lora_amharic_legal\n",
      "  LoRA rank (r): 8\n",
      "  LoRA alpha: 32\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"agkphysics/wav2vec2-large-xlsr-53-amharic\"\n",
    "\n",
    "# Dataset paths - Kaggle Input\n",
    "# Assume Dataset_4.0h is uploaded to Kaggle as input dataset\n",
    "# Update the path below to match your actual Kaggle input dataset name\n",
    "# Kaggle input path format: /kaggle/input/your-dataset-name\n",
    "BASE_DATASET_DIR = \"/kaggle/input/dataset-4-0h/Dataset_4.0h\"  # UPDATE THIS to match your Kaggle dataset name\n",
    "# The dataset should contain: audio/, train.csv, val.csv, test.csv\n",
    "AUDIO_DIR = f\"{BASE_DATASET_DIR}/audio\"\n",
    "TRAIN_CSV = f\"{BASE_DATASET_DIR}/train.csv\"\n",
    "VAL_CSV = f\"{BASE_DATASET_DIR}/val.csv\"\n",
    "TEST_CSV = f\"{BASE_DATASET_DIR}/test.csv\"\n",
    "\n",
    "# Output directory - Kaggle working directory (persistent)\n",
    "OUTPUT_DIR = \"/kaggle/working/wav2vec2_lora_v3\"\n",
    "\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 4,\n",
    "    \"lora_alpha\": 8,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"],\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"FEATURE_EXTRACTION\"\n",
    "}\n",
    "\n",
    "# Training configuration for Kaggle GPU\n",
    "# Parameters match other ablation study notebooks\n",
    "# Checkpoints will be saved every 500 steps to /kaggle/working/ (persistent)\n",
    "TRAINING_ARGS = {\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 1e-4,  # Lowered from 3e-4 for more stable training\n",
    "    \"warmup_steps\": 500,\n",
    "    \"max_steps\": 2000,  # Increased steps for larger dataset\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"fp16\": True,\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"eval_steps\": 500,  # You might want to reduce this too, e.g., 300 or 200\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 500,  # You might want to reduce this too, e.g., 300 or 200\n",
    "    \"save_total_limit\": 3,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"wer\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"logging_steps\": 100,\n",
    "    \"report_to\": \"none\",\n",
    "    \"push_to_hub\": False\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  LoRA rank (r): {LORA_CONFIG['r']}\")\n",
    "print(f\"  LoRA alpha: {LORA_CONFIG['lora_alpha']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SEkOVMoRHVU"
   },
   "source": [
    "## 3. Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kk1eD9YSRHVV",
    "outputId": "2f440748-c623-456a-ec6c-c339d877bad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 302\n",
      "Validation samples: 37\n",
      "Test samples: 39\n",
      "\n",
      "Total samples: 378\n"
     ]
    }
   ],
   "source": [
    "def load_csv_split(csv_path, audio_dir):\n",
    "    \"\"\"Load a CSV split and return list of (audio_path, transcription) tuples\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        audio_path = Path(audio_dir) / row['file_name']\n",
    "        transcription = str(row['transcription']).strip()\n",
    "\n",
    "        if audio_path.exists():\n",
    "            data.append({\n",
    "                'audio_path': str(audio_path),\n",
    "                'transcription': transcription\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Audio file not found: {audio_path}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = load_csv_split(TRAIN_CSV, AUDIO_DIR)\n",
    "val_data = load_csv_split(VAL_CSV, AUDIO_DIR)\n",
    "test_data = load_csv_split(TEST_CSV, AUDIO_DIR)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nTotal samples: {len(train_data) + len(val_data) + len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX4M4CVLRHVW"
   },
   "source": [
    "## 4. Create Vocabulary and Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qW2cL0xRRHVY",
    "outputId": "a45d2ad8-00ff-41ba-d3a2-daf21abba8de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 170\n",
      "First 20 characters: [' ', 'ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ', 'ለ', 'ሉ', 'ሊ', 'ላ', 'ሌ', 'ል', 'ሎ', 'መ', 'ሙ', 'ሚ', 'ማ', 'ሜ']\n"
     ]
    }
   ],
   "source": [
    "# Use the original model's processor instead of creating a new vocabulary\n",
    "# This preserves the pre-trained CTC head weights\n",
    "print(\"Loading original model processor to preserve vocabulary and CTC head...\")\n",
    "original_processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "print(f\"Original vocabulary size: {len(original_processor.tokenizer)}\")\n",
    "print(f\"First 20 characters: {list(original_processor.tokenizer.get_vocab().keys())[:20]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hga5SKM4RHVZ",
    "outputId": "1a718f5e-fe27-4a5e-bccd-2e2cc98c06a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor created successfully\n"
     ]
    }
   ],
   "source": [
    "# Use the original processor to maintain vocabulary compatibility\n",
    "processor = original_processor\n",
    "print(\"Using original model processor to preserve CTC head weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8XvLtU4RHVa"
   },
   "source": [
    "## 5. Load Model and Apply LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yE9J73VPRHVb",
    "outputId": "21410b9c-0ad8-4e1a-80d3-ee5714884c03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at agkphysics/wav2vec2-large-xlsr-53-amharic and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([234]) in the checkpoint and torch.Size([172]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([234, 1024]) in the checkpoint and torch.Size([172, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded: agkphysics/wav2vec2-large-xlsr-53-amharic\n",
      "Vocabulary size: 172\n",
      "Model parameters: 315.62M\n",
      "Model moved to CUDA\n"
     ]
    }
   ],
   "source": [
    "# Load model WITHOUT reinitializing CTC head - use original vocabulary\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id\n",
    "    # No vocab_size or ignore_mismatched_sizes - preserves original CTC head\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded: {MODEL_NAME}\")\n",
    "print(f\"Vocabulary size: {len(processor.tokenizer)}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# Ensure CTC head (lm_head) is trainable\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "print(f\"CTC head (lm_head) parameters: {sum(p.numel() for p in model.lm_head.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"CTC head trainable: {all(p.requires_grad for p in model.lm_head.parameters())}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "    print(\"Model moved to CUDA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APDNpsMwRHVb",
    "outputId": "f55d1e21-a612-448d-d3f6-38a049e9744a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 317,187,884 || trainable%: 0.4959\n",
      "\n",
      "LoRA adapters applied successfully\n",
      "Model base forward method wrapped to filter input_ids and inputs_embeds\n",
      "Applied comprehensive PEFT compatibility patches for Wav2Vec2\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_CONFIG[\"r\"],\n",
    "    lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
    "    target_modules=LORA_CONFIG[\"target_modules\"],\n",
    "    lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
    "    bias=LORA_CONFIG[\"bias\"],\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Ensure CTC head remains trainable after LoRA application\n",
    "for param in model.base_model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Count trainable parameters including CTC head\n",
    "lm_head_params = sum(p.numel() for p in model.base_model.lm_head.parameters() if p.requires_grad)\n",
    "print(f\"\\nCTC head (lm_head) trainable parameters: {lm_head_params / 1e6:.2f}M\")\n",
    "\n",
    "print(\"\\nLoRA adapters applied successfully\")\n",
    "\n",
    "# Wrap forward method to filter out input_ids and inputs_embeds (PEFT adds them but Wav2Vec2ForCTC doesn't need them)\n",
    "original_base_forward = model.base_model.forward\n",
    "\n",
    "def filtered_base_forward(*args, **kwargs):\n",
    "    # Remove input_ids and inputs_embeds if present (PEFT adds them but Wav2Vec2ForCTC doesn't need them)\n",
    "    # Wav2Vec2ForCTC only expects input_values, not input_ids or inputs_embeds\n",
    "    if 'input_ids' in kwargs:\n",
    "        del kwargs['input_ids']\n",
    "    if 'inputs_embeds' in kwargs:\n",
    "        del kwargs['inputs_embeds']\n",
    "    return original_base_forward(*args, **kwargs)\n",
    "\n",
    "model.base_model.forward = filtered_base_forward\n",
    "print(\"Model base forward method wrapped to filter input_ids and inputs_embeds\")\n",
    "\n",
    "# RIGHT AFTER applying LoRA and before creating Trainer\n",
    "# Solution 1: Patch PEFT's save function\n",
    "from peft.utils.save_and_load import get_peft_model_state_dict\n",
    "import peft.utils.save_and_load\n",
    "\n",
    "_original_get_peft_state_dict = get_peft_model_state_dict\n",
    "\n",
    "def safe_get_peft_state_dict(model, state_dict=None, adapter_name=\"default\",\n",
    "                              unwrap_compiled=False, save_embedding_layers=False, **kwargs):\n",
    "    \"\"\"Skip embedding layers for Wav2Vec2 models\"\"\"\n",
    "    if hasattr(model, 'base_model'):\n",
    "        base_class = model.base_model.__class__.__name__\n",
    "        if 'Wav2Vec2' in base_class:\n",
    "            save_embedding_layers = False\n",
    "    elif 'Wav2Vec2' in model.__class__.__name__:\n",
    "        save_embedding_layers = False\n",
    "\n",
    "    return _original_get_peft_state_dict(\n",
    "        model, state_dict=state_dict, adapter_name=adapter_name,\n",
    "        unwrap_compiled=unwrap_compiled,\n",
    "        save_embedding_layers=save_embedding_layers, **kwargs\n",
    "    )\n",
    "\n",
    "peft.utils.save_and_load.get_peft_model_state_dict = safe_get_peft_state_dict\n",
    "\n",
    "# Solution 2: Patch the Wav2Vec2Model class\n",
    "from transformers import Wav2Vec2Model\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2ForCTC\n",
    "\n",
    "Wav2Vec2Model.get_input_embeddings = lambda self: None\n",
    "Wav2Vec2Model.get_output_embeddings = lambda self: None\n",
    "Wav2Vec2Model.set_input_embeddings = lambda self, x: None\n",
    "Wav2Vec2Model.set_output_embeddings = lambda self, x: None\n",
    "\n",
    "Wav2Vec2ForCTC.get_input_embeddings = lambda self: None\n",
    "Wav2Vec2ForCTC.get_output_embeddings = lambda self: (self.lm_head if hasattr(self, 'lm_head') else None)\n",
    "Wav2Vec2ForCTC.set_input_embeddings = lambda self, x: None\n",
    "Wav2Vec2ForCTC.set_output_embeddings = lambda self, x: (setattr(self, 'lm_head', x) if hasattr(self, 'lm_head') else None)\n",
    "\n",
    "print(\"Applied comprehensive PEFT compatibility patches for Wav2Vec2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGjPQteGRHVb"
   },
   "source": [
    "## 6. Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "referenced_widgets": [
      "42649ca8bd0648db8966485a57f81531",
      "517048ac9b714570b114156b67e7f3ba",
      "ed9c593de4b946859093bcefec492105",
      "6def75c803d5403e8c2410633655d31d",
      "49dd8d55eaa44953aab2cb28ccd114e4",
      "76d5deb9356047dc9a4a8e83ec9b1d1a",
      "2c0bdc188df146c1aa93dc2693e9a94a",
      "e083923a7c854c2d90026366abc3112d",
      "c2b90a4443314f66aca7e8ab8e3e5c1e",
      "d30049acfc7a42c8a52ff4a35d255f82",
      "eb20e5586f8146e5b0f2acb4cc1df38e",
      "c5c90634bce04d66bc6d3c1b5edd3eda",
      "5c4329e8541b49fb84a80c28b6390c7a",
      "4c30ca48d6e2409eb660aa319bec31f7",
      "8992eb0fa5a74dc4b75d4649336ff774",
      "2696ada16df2448baea07f7061442f23",
      "62349c8a80bc407fb675ba8e0880198d",
      "d4fde5f445b94a3a9f953c3d4c824e48",
      "617b78b2ff594f1c8d071955947f5848",
      "6ad7ccdc65bb4561b44d0b6e17c011a2",
      "71daaa4fe75e432e9b2997ea0c324c91",
      "70bbbe6d03ee4944bbba199fff1e97f2",
      "3066992bc6a042b9a9d3774d0a49afe9",
      "5522323fea774fc3aa639de383bbc6a6",
      "3fb8e3ed5d474ef48c40bb5ef1b32c53",
      "c7b6988b67084060ad526aac426b783a",
      "51372394c01a4abc9fef41872b11f556",
      "b4e6b622c0d148908090150d8bec7a15",
      "100cbfeb0f9d4987befae78553e3bad4",
      "d746c0494a3c4247b4de3813e32ded61",
      "6b7d4f3fd3f7479aacb62aaf7cea690c",
      "64a07e9bc4b64bedb4b3d9b78aedc0f3",
      "49c1b3c35cd7405ebfc838a237a75a58"
     ]
    },
    "id": "49LM_ofyRHVb",
    "outputId": "9c5aedca-a34b-430f-bc1a-310a3223c622"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42649ca8bd0648db8966485a57f81531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c90634bce04d66bc6d3c1b5edd3eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3066992bc6a042b9a9d3774d0a49afe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepared:\n",
      "  Train: 302 samples\n",
      "  Validation: 37 samples\n",
      "  Test: 39 samples\n"
     ]
    }
   ],
   "source": [
    "def speech_file_to_array_fn(path):\n",
    "    \"\"\"Load audio file and resample to 16kHz\"\"\"\n",
    "    speech_array, sampling_rate = librosa.load(path, sr=16000)\n",
    "    return speech_array\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"Process a batch of audio and transcriptions\"\"\"\n",
    "    audio = [speech_file_to_array_fn(path) for path in batch[\"audio_path\"]]\n",
    "\n",
    "    # Process audio - return as lists (no return_tensors)\n",
    "    audio_features = processor.feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=16000\n",
    "        # No padding, no return_tensors here - collator handles it\n",
    "    )\n",
    "    # Only store input_values, not attention_mask\n",
    "    batch[\"input_values\"] = audio_features.input_values  # This will be a list of arrays\n",
    "\n",
    "    # Process text using tokenizer directly - return as lists\n",
    "    batch[\"labels\"] = [processor.tokenizer(transcription, add_special_tokens=False)[\"input_ids\"] for transcription in batch[\"transcription\"]]\n",
    "\n",
    "    # Make sure we're not accidentally storing attention_mask\n",
    "    if \"attention_mask\" in batch:\n",
    "        del batch[\"attention_mask\"]\n",
    "\n",
    "    return batch\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    batch_size=100,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "print(\"Datasets prepared:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePMnBg7URHVc"
   },
   "source": [
    "## 7. Data Collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "do_OeZxlRHVc",
    "outputId": "772f883b-207b-4a60-c030-9017da9d9696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collator created\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        import torch\n",
    "\n",
    "        # Extract input_values and labels - explicitly ignore attention_mask if present\n",
    "        input_values_list = [feature[\"input_values\"] for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        # Convert audio to tensors and pad manually\n",
    "        input_values_tensors = []\n",
    "        for iv in input_values_list:\n",
    "            if isinstance(iv, torch.Tensor):\n",
    "                input_values_tensors.append(iv)\n",
    "            elif isinstance(iv, np.ndarray):\n",
    "                input_values_tensors.append(torch.tensor(iv, dtype=torch.float32))\n",
    "            else:\n",
    "                input_values_tensors.append(torch.tensor(np.array(iv), dtype=torch.float32))\n",
    "\n",
    "        # Pad audio sequences\n",
    "        input_values = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_values_tensors,\n",
    "            batch_first=True,\n",
    "            padding_value=0.0\n",
    "        )\n",
    "\n",
    "        # Pad labels manually\n",
    "        max_label_len = max(len(labels) for labels in label_features)\n",
    "        pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "\n",
    "        padded_labels = []\n",
    "        for labels in label_features:\n",
    "            if isinstance(labels, torch.Tensor):\n",
    "                labels = labels.tolist()\n",
    "\n",
    "            padding_length = max_label_len - len(labels)\n",
    "            padded = labels + [pad_token_id] * padding_length\n",
    "            padded_labels.append(padded)\n",
    "\n",
    "        labels_tensor = torch.tensor(padded_labels, dtype=torch.long)\n",
    "\n",
    "        # Mask padded labels with -100\n",
    "        attention_mask_labels = (labels_tensor != pad_token_id).long()\n",
    "        labels_tensor = labels_tensor.masked_fill(attention_mask_labels.ne(1), -100)\n",
    "\n",
    "        # Return ONLY input_values and labels - explicitly create a new dict\n",
    "        batch = {}\n",
    "        batch[\"input_values\"] = input_values\n",
    "        batch[\"labels\"] = labels_tensor\n",
    "\n",
    "        # Explicitly ensure no other keys are present\n",
    "        return batch\n",
    "# Add these lines:\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "print(\"Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8Lx1gm2RHVd"
   },
   "source": [
    "## 8. Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C05gDw0RRHVe",
    "outputId": "853faab0-693b-4813-cf20-67df5acc3ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics function created\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute WER (Word Error Rate) and CER (Character Error Rate)\"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = jiwer.wer(label_str, pred_str)\n",
    "    cer = jiwer.cer(label_str, pred_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "print(\"Evaluation metrics function created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9h39rbJRHVe"
   },
   "source": [
    "## 9. Training Arguments and Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kIsrmgrVRHVf",
    "outputId": "c75f944c-4e41-4992-aead-a528012643ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized\n",
      "\n",
      "Training configuration:\n",
      "  Max steps: 1000\n",
      "  Batch size: 4\n",
      "  Gradient accumulation: 4\n",
      "  Effective batch size: 16\n",
      "  Learning rate: 0.0003\n",
      "  Evaluation steps: 500\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(**TRAINING_ARGS)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Evaluation steps: {training_args.eval_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o4oggy-RHVf"
   },
   "source": [
    "## 10. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for Kaggle (persistent)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory created: {OUTPUT_DIR}\")\n",
    "print(f\"Checkpoints will be saved every {TRAINING_ARGS['save_steps']} steps\")\n",
    "print(f\"All outputs will be saved to Kaggle working directory (persistent)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is not wrapped in DataParallel and is on single device\n",
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    print(\"Unwrapping model from DataParallel...\")\n",
    "    model = model.module\n",
    "\n",
    "# Move to single device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model on device: {device}\")\n",
    "print(f\"Visible GPUs: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "-EAFlSv6RHVg",
    "outputId": "a7cd9a1e-dc9d-4088-ac3d-0e51d800df65"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 2:07:43, Epoch 52/53]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.693900</td>\n",
       "      <td>5.121708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.792800</td>\n",
       "      <td>4.608199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.924718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "Training loss: 10.1412\n",
      "Final model saved to: wav2vec2_lora_amharic_legal_final\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "\n",
    "# Save final model\n",
    "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
    "trainer.save_model(final_model_path)\n",
    "processor.save_pretrained(final_model_path)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Final model saved to: {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viz_markdown"
   },
   "source": [
    "## Training Visualizations\n",
    "\n",
    "Visualize training progress, validation metrics, and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Extract training history from trainer state\n",
    "train_history = trainer.state.log_history\n",
    "\n",
    "# Separate training and evaluation logs\n",
    "train_logs = [log for log in train_history if 'loss' in log and 'eval_loss' not in log]\n",
    "eval_logs = [log for log in train_history if 'eval_loss' in log]\n",
    "\n",
    "# Extract data\n",
    "train_steps = [log['step'] for log in train_logs]\n",
    "train_losses = [log['loss'] for log in train_logs]\n",
    "\n",
    "eval_steps = [log['step'] for log in eval_logs]\n",
    "eval_losses = [log['eval_loss'] for log in eval_logs]\n",
    "eval_wers = [log.get('eval_wer', 0) for log in eval_logs]\n",
    "eval_cers = [log.get('eval_cer', 0) for log in eval_logs]\n",
    "\n",
    "# Create comprehensive figure\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Training Loss (top left, spans 2 columns)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.plot(train_steps, train_losses, 'b-', linewidth=2.5, label='Training Loss', alpha=0.8)\n",
    "ax1.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('Training Loss Over Time', fontsize=15, fontweight='bold', pad=15)\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "ax1.legend(fontsize=12, loc='best')\n",
    "ax1.set_facecolor('#f8f9fa')\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "if eval_losses:\n",
    "    ax2.plot(eval_steps, eval_losses, 'r-', linewidth=2.5, marker='o', markersize=5, label='Validation Loss', alpha=0.8)\n",
    "    ax2.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No validation data', ha='center', va='center', fontsize=12)\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 3: Word Error Rate (WER)\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "if eval_wers and any(w > 0 for w in eval_wers):\n",
    "    ax3.plot(eval_steps, eval_wers, 'g-', linewidth=2.5, marker='s', markersize=5, label='WER', alpha=0.8)\n",
    "    ax3.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Word Error Rate', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Word Error Rate (WER)', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.set_ylim(bottom=0)\n",
    "    ax3.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No WER data', ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Word Error Rate (WER)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Character Error Rate (CER)\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "if eval_cers and any(c > 0 for c in eval_cers):\n",
    "    ax4.plot(eval_steps, eval_cers, 'm-', linewidth=2.5, marker='^', markersize=5, label='CER', alpha=0.8)\n",
    "    ax4.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Character Error Rate', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Character Error Rate (CER)', fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax4.legend(fontsize=11)\n",
    "    ax4.set_ylim(bottom=0)\n",
    "    ax4.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No CER data', ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('Character Error Rate (CER)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 5: Combined Loss Plot (Training vs Validation)\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "if eval_losses:\n",
    "    ax5.plot(train_steps, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.7)\n",
    "    ax5.plot(eval_steps, eval_losses, 'r-', linewidth=2, marker='o', markersize=4, label='Validation Loss', alpha=0.7)\n",
    "    ax5.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax5.legend(fontsize=11)\n",
    "    ax5.set_facecolor('#f8f9fa')\n",
    "else:\n",
    "    ax5.plot(train_steps, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.7)\n",
    "    ax5.set_xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax5.legend(fontsize=11)\n",
    "    ax5.set_facecolor('#f8f9fa')\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f'Training Progress: {OUTPUT_DIR}', fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()\n",
    "\n",
    "# Save high-resolution plot\n",
    "plot_path = f\"{OUTPUT_DIR}_training_plots.png\"\n",
    "fig.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"\\nTraining plots saved to: {plot_path}\")\n",
    "\n",
    "# Print detailed summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING SUMMARY: {OUTPUT_DIR}\")\n",
    "print(\"=\"*70)\n",
    "if train_losses:\n",
    "    print(f\"\\nTraining Loss:\")\n",
    "    print(f\"  Initial: {train_losses[0]:.4f}\")\n",
    "    print(f\"  Final: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  Best: {min(train_losses):.4f} (at step {train_steps[train_losses.index(min(train_losses))]})\")\n",
    "    print(f\"  Improvement: {((train_losses[0] - min(train_losses)) / train_losses[0] * 100):.2f}%\")\n",
    "if eval_losses:\n",
    "    print(f\"\\nValidation Loss:\")\n",
    "    print(f\"  Initial: {eval_losses[0]:.4f}\")\n",
    "    print(f\"  Final: {eval_losses[-1]:.4f}\")\n",
    "    print(f\"  Best: {min(eval_losses):.4f} (at step {eval_steps[eval_losses.index(min(eval_losses))]})\")\n",
    "    print(f\"  Improvement: {((eval_losses[0] - min(eval_losses)) / eval_losses[0] * 100):.2f}%\")\n",
    "if eval_wers and any(w > 0 for w in eval_wers):\n",
    "    valid_wers = [w for w in eval_wers if w > 0]\n",
    "    valid_steps = [eval_steps[i] for i, w in enumerate(eval_wers) if w > 0]\n",
    "    print(f\"\\nWord Error Rate (WER):\")\n",
    "    print(f\"  Initial: {valid_wers[0]:.4f}\")\n",
    "    print(f\"  Final: {valid_wers[-1]:.4f}\")\n",
    "    print(f\"  Best: {min(valid_wers):.4f} (at step {valid_steps[valid_wers.index(min(valid_wers))]})\")\n",
    "    print(f\"  Improvement: {((valid_wers[0] - min(valid_wers)) / valid_wers[0] * 100):.2f}%\")\n",
    "if eval_cers and any(c > 0 for c in eval_cers):\n",
    "    valid_cers = [c for c in eval_cers if c > 0]\n",
    "    valid_steps = [eval_steps[i] for i, c in enumerate(eval_cers) if c > 0]\n",
    "    print(f\"\\nCharacter Error Rate (CER):\")\n",
    "    print(f\"  Initial: {valid_cers[0]:.4f}\")\n",
    "    print(f\"  Final: {valid_cers[-1]:.4f}\")\n",
    "    print(f\"  Best: {min(valid_cers):.4f} (at step {valid_steps[valid_cers.index(min(valid_cers))]})\")\n",
    "    print(f\"  Improvement: {((valid_cers[0] - min(valid_cers)) / valid_cers[0] * 100):.2f}%\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Loss Plot (Epoch-based)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract training history from trainer state\n",
    "train_history = trainer.state.log_history\n",
    "\n",
    "# Separate training and evaluation logs\n",
    "train_logs = [log for log in train_history if 'loss' in log and 'eval_loss' not in log]\n",
    "eval_logs = [log for log in train_history if 'eval_loss' in log]\n",
    "\n",
    "# Extract epochs and losses\n",
    "# Group training logs by epoch\n",
    "train_by_epoch = {}\n",
    "for log in train_logs:\n",
    "    epoch = log.get('epoch', 0)\n",
    "    if epoch not in train_by_epoch:\n",
    "        train_by_epoch[epoch] = []\n",
    "    train_by_epoch[epoch].append(log.get('loss', 0))\n",
    "\n",
    "# Average losses per epoch\n",
    "epochs = sorted(train_by_epoch.keys())\n",
    "train_losses = [np.mean(train_by_epoch[epoch]) for epoch in epochs]\n",
    "\n",
    "# Extract validation losses by epoch\n",
    "eval_by_epoch = {}\n",
    "for log in eval_logs:\n",
    "    epoch = log.get('epoch', 0)\n",
    "    if epoch not in eval_by_epoch:\n",
    "        eval_by_epoch[epoch] = []\n",
    "    eval_by_epoch[epoch].append(log.get('eval_loss', 0))\n",
    "\n",
    "# Average validation losses per epoch\n",
    "eval_epochs = sorted(eval_by_epoch.keys())\n",
    "eval_losses = [np.mean(eval_by_epoch[epoch]) for epoch in eval_epochs]\n",
    "\n",
    "# Create the plot matching the image style\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, 'b-', linewidth=2, label='Training Loss', alpha=0.8)\n",
    "if eval_epochs:\n",
    "    plt.plot(eval_epochs, eval_losses, 'orange', linewidth=2, label='Validation Loss', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "plt.title('Training and Validation Loss', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plot\n",
    "plot_path = f\"{OUTPUT_DIR}_loss_epochs.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print(f\"Epoch-based loss plot saved to: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQUJNm5eRHVh"
   },
   "source": [
    "## 11. Save Model (Kaggle Output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAwyb1zARHVi",
    "outputId": "faae84ff-5eea-4886-ced9-afee305bad9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating zip file: wav2vec2_lora_amharic_legal_final.zip...\n",
      "  Added: added_tokens.json\n",
      "  Added: preprocessor_config.json\n",
      "  Added: special_tokens_map.json\n",
      "  Added: tokenizer_config.json\n",
      "  Added: adapter_config.json\n",
      "  Added: training_args.bin\n",
      "  Added: adapter_model.safetensors\n",
      "  Added: README.md\n",
      "  Added: vocab.json\n",
      "\n",
      "Zip file created: wav2vec2_lora_amharic_legal_final.zip\n",
      "Model zip file copied to Google Drive: /content/drive/MyDrive/wav2vec2_lora_amharic_legal_final.zip\n",
      "\n",
      "File size: 6.21 MB\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Zip the final model directory (saved to Kaggle working directory)\n",
    "zip_filename = f\"/kaggle/working/{Path(final_model_path).name}.zip\"\n",
    "print(f\"Creating zip file: {zip_filename}...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file_path in Path(final_model_path).rglob(\"*\"):\n",
    "        if file_path.is_file():\n",
    "            # Get relative path for archive\n",
    "            arcname = file_path.relative_to(final_model_path)\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\"  Added: {arcname}\")\n",
    "\n",
    "print(f\"\\nZip file created: {zip_filename}\")\n",
    "print(f\"File size: {Path(zip_filename).stat().st_size / (1024*1024):.2f} MB\")\n",
    "print(f\"\\nModel and zip file saved to Kaggle working directory (persistent)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrA-9_3CRHVj"
   },
   "source": [
    "## 12. Final Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZAuL4OgRHVk",
    "outputId": "4bf62d8b-38ff-4d74-f942-87dc288c582b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved to: wav2vec2_lora_amharic_legal_final\n",
      "\n",
      "Files saved:\n",
      "  - LoRA adapters (adapter_model.bin, adapter_config.json)\n",
      "  - Processor (tokenizer, feature_extractor)\n",
      "  - Training configuration\n"
     ]
    }
   ],
   "source": [
    "final_model_path = f\"{OUTPUT_DIR}_final\"\n",
    "trainer.save_model(final_model_path)\n",
    "processor.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - LoRA adapters (adapter_model.bin, adapter_config.json)\")\n",
    "print(\"  - Processor (tokenizer, feature_extractor)\")\n",
    "print(\"  - Training configuration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbojTaxzRHVl"
   },
   "source": [
    "## 13. Inference Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNTgA1skRHVl",
    "outputId": "c0eb1b89-eabf-4cb3-ddb7-8b56066741c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference function created\n",
      "\n",
      "Example usage:\n",
      "  transcription = transcribe_audio(model, processor, 'path/to/audio.mp3')\n",
      "  print(transcription)\n"
     ]
    }
   ],
   "source": [
    "def transcribe_audio(model, processor, audio_path):\n",
    "    \"\"\"Transcribe a single audio file\"\"\"\n",
    "    speech, _ = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    inputs = processor(\n",
    "        speech,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return transcription\n",
    "\n",
    "print(\"Inference function created\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  transcription = transcribe_audio(model, processor, 'path/to/audio.mp3')\")\n",
    "print(\"  print(transcription)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB2IActjRHVl"
   },
   "source": [
    "## 14. Load Model for Inference (After Training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuLQLT2URHVn",
    "outputId": "3cd4b42c-748a-4dd7-8567-42c72852f5da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading function created\n",
      "\n",
      "Example usage:\n",
      "  model, processor = load_trained_model(\n",
      "      MODEL_NAME,\n",
      "      'wav2vec2_lora_amharic_legal_final',\n",
      "      'wav2vec2_lora_amharic_legal_final'\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def load_trained_model(base_model_name, adapter_path, processor_path):\n",
    "    \"\"\"Load base model and LoRA adapters\"\"\"\n",
    "    processor = Wav2Vec2Processor.from_pretrained(processor_path)\n",
    "\n",
    "    base_model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        base_model_name,\n",
    "        ctc_loss_reduction=\"mean\",\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        vocab_size=len(processor.tokenizer)\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    return model, processor\n",
    "\n",
    "print(\"Model loading function created\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  model, processor = load_trained_model(\")\n",
    "print(\"      MODEL_NAME,\")\n",
    "print(f\"      '{OUTPUT_DIR}_final',\")\n",
    "print(f\"      '{OUTPUT_DIR}_final'\")\n",
    "print(\"  )\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "100cbfeb0f9d4987befae78553e3bad4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2696ada16df2448baea07f7061442f23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c0bdc188df146c1aa93dc2693e9a94a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3066992bc6a042b9a9d3774d0a49afe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5522323fea774fc3aa639de383bbc6a6",
       "IPY_MODEL_3fb8e3ed5d474ef48c40bb5ef1b32c53",
       "IPY_MODEL_c7b6988b67084060ad526aac426b783a"
      ],
      "layout": "IPY_MODEL_51372394c01a4abc9fef41872b11f556"
     }
    },
    "3fb8e3ed5d474ef48c40bb5ef1b32c53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d746c0494a3c4247b4de3813e32ded61",
      "max": 39,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6b7d4f3fd3f7479aacb62aaf7cea690c",
      "value": 39
     }
    },
    "42649ca8bd0648db8966485a57f81531": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_517048ac9b714570b114156b67e7f3ba",
       "IPY_MODEL_ed9c593de4b946859093bcefec492105",
       "IPY_MODEL_6def75c803d5403e8c2410633655d31d"
      ],
      "layout": "IPY_MODEL_49dd8d55eaa44953aab2cb28ccd114e4"
     }
    },
    "49c1b3c35cd7405ebfc838a237a75a58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49dd8d55eaa44953aab2cb28ccd114e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c30ca48d6e2409eb660aa319bec31f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_617b78b2ff594f1c8d071955947f5848",
      "max": 37,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6ad7ccdc65bb4561b44d0b6e17c011a2",
      "value": 37
     }
    },
    "51372394c01a4abc9fef41872b11f556": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "517048ac9b714570b114156b67e7f3ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76d5deb9356047dc9a4a8e83ec9b1d1a",
      "placeholder": "​",
      "style": "IPY_MODEL_2c0bdc188df146c1aa93dc2693e9a94a",
      "value": "Map: 100%"
     }
    },
    "5522323fea774fc3aa639de383bbc6a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4e6b622c0d148908090150d8bec7a15",
      "placeholder": "​",
      "style": "IPY_MODEL_100cbfeb0f9d4987befae78553e3bad4",
      "value": "Map: 100%"
     }
    },
    "5c4329e8541b49fb84a80c28b6390c7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62349c8a80bc407fb675ba8e0880198d",
      "placeholder": "​",
      "style": "IPY_MODEL_d4fde5f445b94a3a9f953c3d4c824e48",
      "value": "Map: 100%"
     }
    },
    "617b78b2ff594f1c8d071955947f5848": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62349c8a80bc407fb675ba8e0880198d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64a07e9bc4b64bedb4b3d9b78aedc0f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ad7ccdc65bb4561b44d0b6e17c011a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b7d4f3fd3f7479aacb62aaf7cea690c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6def75c803d5403e8c2410633655d31d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d30049acfc7a42c8a52ff4a35d255f82",
      "placeholder": "​",
      "style": "IPY_MODEL_eb20e5586f8146e5b0f2acb4cc1df38e",
      "value": " 302/302 [00:10&lt;00:00, 34.12 examples/s]"
     }
    },
    "70bbbe6d03ee4944bbba199fff1e97f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71daaa4fe75e432e9b2997ea0c324c91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76d5deb9356047dc9a4a8e83ec9b1d1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8992eb0fa5a74dc4b75d4649336ff774": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71daaa4fe75e432e9b2997ea0c324c91",
      "placeholder": "​",
      "style": "IPY_MODEL_70bbbe6d03ee4944bbba199fff1e97f2",
      "value": " 37/37 [00:00&lt;00:00, 54.77 examples/s]"
     }
    },
    "b4e6b622c0d148908090150d8bec7a15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2b90a4443314f66aca7e8ab8e3e5c1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c5c90634bce04d66bc6d3c1b5edd3eda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5c4329e8541b49fb84a80c28b6390c7a",
       "IPY_MODEL_4c30ca48d6e2409eb660aa319bec31f7",
       "IPY_MODEL_8992eb0fa5a74dc4b75d4649336ff774"
      ],
      "layout": "IPY_MODEL_2696ada16df2448baea07f7061442f23"
     }
    },
    "c7b6988b67084060ad526aac426b783a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64a07e9bc4b64bedb4b3d9b78aedc0f3",
      "placeholder": "​",
      "style": "IPY_MODEL_49c1b3c35cd7405ebfc838a237a75a58",
      "value": " 39/39 [00:00&lt;00:00, 41.89 examples/s]"
     }
    },
    "d30049acfc7a42c8a52ff4a35d255f82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4fde5f445b94a3a9f953c3d4c824e48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d746c0494a3c4247b4de3813e32ded61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e083923a7c854c2d90026366abc3112d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb20e5586f8146e5b0f2acb4cc1df38e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed9c593de4b946859093bcefec492105": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e083923a7c854c2d90026366abc3112d",
      "max": 302,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2b90a4443314f66aca7e8ab8e3e5c1e",
      "value": 302
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
