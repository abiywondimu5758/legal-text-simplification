{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Amharic Legal RAG System\n",
        "\n",
        "This notebook builds a retrieval-augmented system for Amharic legal documents. \n",
        "\n",
        "**Purpose**: Provide contextual grounding for legal text simplification, NOT generation.\n",
        "\n",
        "**Key Features**:\n",
        "- Article-level chunking (አንቀጽ)\n",
        "- Gemini embeddings for Amharic\n",
        "- Rich legal metadata\n",
        "- Retrieval-only (no generation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working directory: /Users/blank/Documents/Foundation Models Course Projects\n",
            "Directory exists: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Set working directory to project root\n",
        "WORKDIR = Path(\"/Users/blank/Documents/Foundation Models Course Projects\")\n",
        "os.chdir(WORKDIR)\n",
        "\n",
        "print(f\"Working directory: {WORKDIR}\")\n",
        "print(f\"Directory exists: {WORKDIR.exists()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Dependency Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q pymupdf faiss-cpu pandas tqdm pyarrow google-generativeai numpy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PDF directory: /Users/blank/Documents/Foundation Models Course Projects/Dataset/all_pdfs\n",
            "PDF directory exists: True\n",
            "Gemini API key loaded from file\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LegalRagConfig(raw_pdf_dir=PosixPath('/Users/blank/Documents/Foundation Models Course Projects/Dataset/all_pdfs'), extracted_dir=PosixPath('/Users/blank/Documents/Foundation Models Course Projects/rag_pipeline/1_extracted'), normalized_dir=PosixPath('/Users/blank/Documents/Foundation Models Course Projects/rag_pipeline/2_normalized'), chunked_dir=PosixPath('/Users/blank/Documents/Foundation Models Course Projects/rag_pipeline/3_chunked'), index_path=PosixPath('/Users/blank/Documents/Foundation Models Course Projects/rag_pipeline/4_vector_db/faiss_index.bin'), metadata_path=PosixPath('/Users/blank/Documents/Foundation Models Course Projects/rag_pipeline/4_vector_db/metadata.parquet'), ocr_map_path=PosixPath('/Users/blank/Documents/Foundation Models Course Projects/Dataset/scripts/glossary/ocr_normalization_map.py'), gemini_api_key='REPLACE WITH YOUR API KEY HERE', gemini_embedding_model='models/text-embedding-004', batch_size_docs=None, embedding_batch_size=100, max_chunk_size_bytes=30000000, max_chunk_chars=10000000, top_k_retrieval=2)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class LegalRagConfig:\n",
        "    # Raw data paths\n",
        "    raw_pdf_dir: Path = WORKDIR / \"Dataset\" / \"all_pdfs\"\n",
        "    \n",
        "    # Processing pipeline paths (all under rag_pipeline folder)\n",
        "    extracted_dir: Path = WORKDIR / \"rag_pipeline\" / \"1_extracted\"\n",
        "    normalized_dir: Path = WORKDIR / \"rag_pipeline\" / \"2_normalized\"\n",
        "    chunked_dir: Path = WORKDIR / \"rag_pipeline\" / \"3_chunked\"\n",
        "    \n",
        "    # Vector database\n",
        "    index_path: Path = WORKDIR / \"rag_pipeline\" / \"4_vector_db\" / \"faiss_index.bin\"\n",
        "    metadata_path: Path = WORKDIR / \"rag_pipeline\" / \"4_vector_db\" / \"metadata.parquet\"\n",
        "    \n",
        "    # OCR normalization map path\n",
        "    ocr_map_path: Path = WORKDIR / \"Dataset\" / \"scripts\" / \"glossary\" / \"ocr_normalization_map.py\"\n",
        "    \n",
        "    # Gemini API configuration\n",
        "    gemini_api_key: str = \"\"  # Set here or use GEMINI_API_KEY environment variable\n",
        "    gemini_embedding_model: str = \"models/text-embedding-004\"\n",
        "    \n",
        "    # Processing parameters\n",
        "    batch_size_docs: Optional[int] = None  # None = process all PDFs, or set a number to limit\n",
        "    embedding_batch_size: int = 100  # Chunks per Gemini embedding API call\n",
        "    \n",
        "    # Chunk size limits (to avoid API payload size errors)\n",
        "    max_chunk_size_bytes: int = 30_000_000  # ~30MB max per chunk (Gemini limit is 40MB, but we leave buffer)\n",
        "    max_chunk_chars: int = 10_000_000  # ~10M characters max per chunk\n",
        "    \n",
        "    # Retrieval parameters\n",
        "    top_k_retrieval: int = 2  # Hard limit: 2 chunks for simplification (max 3 only if sentence has conditions+exceptions)\n",
        "\n",
        "cfg = LegalRagConfig()\n",
        "\n",
        "# Create all necessary directories\n",
        "for path in [\n",
        "    cfg.extracted_dir,\n",
        "    cfg.normalized_dir,\n",
        "    cfg.chunked_dir,\n",
        "    cfg.index_path.parent,\n",
        "]:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"PDF directory: {cfg.raw_pdf_dir}\")\n",
        "print(f\"PDF directory exists: {cfg.raw_pdf_dir.exists()}\")\n",
        "\n",
        "# Load Gemini API key from file or environment\n",
        "def load_api_key_from_file(filepath: Path = None) -> str:\n",
        "    \"\"\"Load API key from .gemini_api_key file.\"\"\"\n",
        "    if filepath is None:\n",
        "        possible_paths = [\n",
        "            WORKDIR / \".gemini_api_key\",\n",
        "            WORKDIR / \"gemini_api_key.txt\",\n",
        "            Path.home() / \".gemini_api_key\",\n",
        "        ]\n",
        "        for path in possible_paths:\n",
        "            if path.exists():\n",
        "                try:\n",
        "                    content = path.read_text().strip()\n",
        "                    if \"=\" in content:\n",
        "                        key = content.split(\"=\", 1)[1].strip().strip('\"').strip(\"'\")\n",
        "                    else:\n",
        "                        key = content.strip()\n",
        "                    return key\n",
        "                except Exception as e:\n",
        "                    print(f\"[WARN] Error reading API key file {path}: {e}\")\n",
        "                    continue\n",
        "    return \"\"\n",
        "\n",
        "# Load API key\n",
        "if not cfg.gemini_api_key:\n",
        "    cfg.gemini_api_key = load_api_key_from_file()\n",
        "    if cfg.gemini_api_key:\n",
        "        print(\"Gemini API key loaded from file\")\n",
        "    else:\n",
        "        cfg.gemini_api_key = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
        "        if cfg.gemini_api_key:\n",
        "            print(\"Gemini API key found in environment variable\")\n",
        "        else:\n",
        "            print(\"WARNING: Gemini API key not found. Please create .gemini_api_key file or set GEMINI_API_KEY environment variable\")\n",
        "\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 93 OCR normalization rules\n",
            "Found 94 PDF files\n",
            "Already processed: 94 files\n",
            "Pending to process: 0 files\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting & Normalizing: 0file [00:00, ?file/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracted & normalized 0 documents\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import hashlib\n",
        "from typing import Dict, List\n",
        "import fitz  # pymupdf\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Load OCR normalization map\n",
        "def load_ocr_map() -> Dict[str, str]:\n",
        "    \"\"\"Load OCR normalization map from the existing script.\"\"\"\n",
        "    try:\n",
        "        import importlib.util\n",
        "        spec = importlib.util.spec_from_file_location(\"ocr_map\", cfg.ocr_map_path)\n",
        "        ocr_module = importlib.util.module_from_spec(spec)\n",
        "        spec.loader.exec_module(ocr_module)\n",
        "        return getattr(ocr_module, \"OCR_MAP\", {})\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not load OCR map: {e}\")\n",
        "        return {}\n",
        "\n",
        "OCR_MAP = load_ocr_map()\n",
        "print(f\"Loaded {len(OCR_MAP)} OCR normalization rules\")\n",
        "\n",
        "def apply_ocr_normalization(text: str) -> str:\n",
        "    \"\"\"Apply OCR normalization to text.\"\"\"\n",
        "    normalized = text\n",
        "    for src, dst in OCR_MAP.items():\n",
        "        normalized = normalized.replace(src, dst)\n",
        "    return normalized\n",
        "\n",
        "def normalize_geez_numerals(text: str) -> str:\n",
        "    \"\"\"Convert Ge'ez numerals to Arabic numerals.\"\"\"\n",
        "    # Ge'ez numeral mapping (basic)\n",
        "    geez_to_arabic = {\n",
        "        \"፩\": \"1\", \"፪\": \"2\", \"፫\": \"3\", \"፬\": \"4\", \"፭\": \"5\",\n",
        "        \"፮\": \"6\", \"፯\": \"7\", \"፰\": \"8\", \"፱\": \"9\", \"፲\": \"10\",\n",
        "    }\n",
        "    normalized = text\n",
        "    for geez, arabic in geez_to_arabic.items():\n",
        "        normalized = normalized.replace(geez, arabic)\n",
        "    return normalized\n",
        "\n",
        "def clean_legal_text(text: str) -> str:\n",
        "    \"\"\"Clean legal text: remove headers, footers, page numbers, English content.\"\"\"\n",
        "    # Remove common headers/footers\n",
        "    drop_patterns = [\n",
        "        r\"ፌዴራል ነጋሪት ጋዜጣ\",\n",
        "        r\"የኢትዮጵያ ፌዴራላዊ ዲሞክራሲያዊ ሪፐብሊክ\",\n",
        "        r\"ሕዝብ ተወካዮች ምክር ቤት\",\n",
        "        r\"አዋጅ ቁጥር\",\n",
        "        r\"ገጽ\\s*\\d+\",  # Page numbers\n",
        "        r\"ዓመት ቁጥር\",\n",
        "    ]\n",
        "    \n",
        "    cleaned = text\n",
        "    for pattern in drop_patterns:\n",
        "        cleaned = re.sub(pattern, \"\", cleaned)\n",
        "    \n",
        "    # Remove lines that are primarily English\n",
        "    lines = cleaned.split(\"\\n\")\n",
        "    amharic_lines = []\n",
        "    ethiopic_re = re.compile(r\"[\\u1200-\\u137F\\u1380-\\u139F\\u2D80-\\u2DDF\\uAB00-\\uAB2F]\")\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        # Keep only lines with Ethiopic characters\n",
        "        if ethiopic_re.search(line):\n",
        "            amharic_lines.append(line)\n",
        "    \n",
        "    # Reconstruct text\n",
        "    cleaned = \"\\n\".join(amharic_lines)\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    cleaned = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned)\n",
        "    cleaned = re.sub(r\"\\s+\", \" \", cleaned)\n",
        "    \n",
        "    return cleaned.strip()\n",
        "\n",
        "def extract_pdf_text(pdf_path: Path) -> str:\n",
        "    \"\"\"Extract text from PDF using pymupdf.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(str(pdf_path))\n",
        "        text_parts = []\n",
        "        for page in doc:\n",
        "            text = page.get_text(\"text\")\n",
        "            if text:\n",
        "                text_parts.append(text)\n",
        "        doc.close()\n",
        "        return \"\\n\".join(text_parts)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to extract {pdf_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_file_hash(filepath: Path) -> str:\n",
        "    \"\"\"Compute MD5 hash of file content.\"\"\"\n",
        "    hash_md5 = hashlib.md5()\n",
        "    try:\n",
        "        with open(filepath, \"rb\") as f:\n",
        "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "                hash_md5.update(chunk)\n",
        "        return hash_md5.hexdigest()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def extract_and_normalize_pdfs(max_docs: int = None) -> List[Dict]:\n",
        "    \"\"\"Extract and normalize PDFs. Returns list of processed documents.\"\"\"\n",
        "    # Find all PDFs\n",
        "    pdfs = sorted(cfg.raw_pdf_dir.glob(\"*.pdf\"))\n",
        "    print(f\"Found {len(pdfs)} PDF files\")\n",
        "    \n",
        "    # Check already processed files\n",
        "    processed_paths = set()\n",
        "    extracted_files = sorted(cfg.extracted_dir.glob(\"*.json\"))\n",
        "    \n",
        "    for json_file in extracted_files:\n",
        "        try:\n",
        "            payload = json.loads(json_file.read_text(encoding=\"utf-8\"))\n",
        "            source_path = payload.get(\"source_path\")\n",
        "            if source_path:\n",
        "                processed_paths.add(str(Path(source_path).resolve()))\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    print(f\"Already processed: {len(processed_paths)} files\")\n",
        "    \n",
        "    # Filter pending files\n",
        "    pending = []\n",
        "    for pdf in pdfs:\n",
        "        if str(pdf.resolve()) not in processed_paths:\n",
        "            pending.append(pdf)\n",
        "    \n",
        "    print(f\"Pending to process: {len(pending)} files\")\n",
        "    \n",
        "    # Apply limit\n",
        "    if max_docs is not None:\n",
        "        pending = pending[:max_docs]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for pdf_path in tqdm(pending, desc=\"Extracting & Normalizing\", unit=\"file\"):\n",
        "        try:\n",
        "            # Extract raw text\n",
        "            raw_text = extract_pdf_text(pdf_path)\n",
        "            if not raw_text or len(raw_text.strip()) < 100:\n",
        "                print(f\"[WARN] {pdf_path.name} has insufficient text, skipping\")\n",
        "                continue\n",
        "            \n",
        "            # Clean text\n",
        "            cleaned_text = clean_legal_text(raw_text)\n",
        "            \n",
        "            # Apply OCR normalization\n",
        "            normalized_text = apply_ocr_normalization(cleaned_text)\n",
        "            \n",
        "            # Normalize numerals\n",
        "            normalized_text = normalize_geez_numerals(normalized_text)\n",
        "            \n",
        "            # Save extracted text\n",
        "            extracted_path = cfg.extracted_dir / f\"{pdf_path.stem}.json\"\n",
        "            payload = {\n",
        "                \"source_path\": str(pdf_path),\n",
        "                \"source_title\": pdf_path.stem,\n",
        "                \"raw_text\": raw_text,\n",
        "                \"cleaned_text\": cleaned_text,\n",
        "                \"normalized_text\": normalized_text,\n",
        "                \"file_hash\": get_file_hash(pdf_path),\n",
        "            }\n",
        "            extracted_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "            \n",
        "            # Save normalized text\n",
        "            normalized_path = cfg.normalized_dir / f\"{pdf_path.stem}.txt\"\n",
        "            normalized_path.write_text(normalized_text, encoding=\"utf-8\")\n",
        "            \n",
        "            results.append({\n",
        "                \"source\": str(pdf_path),\n",
        "                \"source_title\": pdf_path.stem,\n",
        "                \"extracted\": str(extracted_path),\n",
        "                \"normalized\": str(normalized_path),\n",
        "                \"text_length\": len(normalized_text),\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed to process {pdf_path}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\nExtracted & normalized {len(results)} documents\")\n",
        "    return results\n",
        "\n",
        "# Run extraction\n",
        "extraction_results = extract_and_normalize_pdfs(max_docs=cfg.batch_size_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chunking Articles:  16%|█▌        | 15/94 [00:00<00:00, 120.59doc/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] No articles detected in 1356, treating as single chunk\n",
            "[WARN] No articles detected in 1364, treating as single chunk\n",
            "[WARN] No articles detected in 1378, treating as single chunk\n",
            "[WARN] No articles detected in Criminal Code (New) (Amharic), treating as single chunk\n",
            "[WARN] No articles detected in civil-procedure-module-revised, treating as single chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chunking Articles:  43%|████▎     | 40/94 [00:00<00:00, 73.91doc/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] No articles detected in federal-supreme-court-decisions-volume-1-2-3, treating as single chunk\n",
            "[WARN] No articles detected in federal-supreme-court-decisions-volume-4, treating as single chunk\n",
            "[WARN] No articles detected in federal-supreme-court-decisions-volume-5, treating as single chunk\n",
            "[WARN] No articles detected in federal-supreme-court-decisions-volume-6, treating as single chunk\n",
            "[WARN] No articles detected in federal-supreme-court-decisions-volume-7, treating as single chunk\n",
            "[WARN] No articles detected in labour-law, treating as single chunk\n",
            "[WARN] No articles detected in module-on-professional-ethics, treating as single chunk\n",
            "[WARN] No articles detected in modules-on-criminal-procedure-law, treating as single chunk\n",
            "[WARN] No articles detected in principles-for-the-judiciary, treating as single chunk\n",
            "[WARN] No articles detected in revised-criminal-procedure-module, treating as single chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chunking Articles:  70%|███████   | 66/94 [00:00<00:00, 123.62doc/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] No articles detected in revised-criminal-procedure-module1, treating as single chunk\n",
            "[WARN] No articles detected in revised-module-on-the-justice-system-and-the-role-of-justice-organs1, treating as single chunk\n",
            "[WARN] No articles detected in the-role-of-prosecutor-in-criminal-investigation, treating as single chunk\n",
            "[WARN] No articles detected in trial-and-pre-trial-managment, treating as single chunk\n",
            "[WARN] No articles detected in trial-and-pre-trial, treating as single chunk\n",
            "[WARN] No articles detected in volume 1-3, treating as single chunk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chunking Articles: 100%|██████████| 94/94 [00:00<00:00, 101.35doc/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] No articles detected in volume 4, treating as single chunk\n",
            "[WARN] No articles detected in volume 5, treating as single chunk\n",
            "[WARN] No articles detected in volume 6, treating as single chunk\n",
            "[WARN] No articles detected in volume 7, treating as single chunk\n",
            "[WARN] No articles detected in አዋጅ-ቁጥር-1354, treating as single chunk\n",
            "Saved chunk manifest with 1228 chunks → /Users/blank/Documents/Foundation Models Course Projects/rag_pipeline/3_chunked/chunk_manifest.parquet\n",
            "\n",
            "Chunking Statistics:\n",
            "  Total chunks: 1228\n",
            "  Unique documents: 65\n",
            "  Domains: {'commercial': 793, 'judicial': 324, 'unknown': 80, 'criminal': 15, 'labor': 14, 'procedure': 2}\n",
            "  Legal functions: {'other': 402, 'obligation': 292, 'definition': 248, 'permission': 157, 'condition': 127, 'prohibition': 2}\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "def detect_article_structure(text: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Detect articles (አንቀጽ) and sub-articles (ንዑስ አንቀጽ) in legal text.\n",
        "    Returns list of article chunks with metadata.\n",
        "    \"\"\"\n",
        "    articles = []\n",
        "    \n",
        "    # Enhanced patterns for article detection - multiple formats\n",
        "    # Pattern 1: Standard \"አንቀጽ\" with number\n",
        "    # Pattern 2: \"አንቀጽ\" with \"ቁጥር\" (number)\n",
        "    # Pattern 3: Just number at start of line (common in some formats)\n",
        "    # Pattern 4: Article with parentheses or brackets\n",
        "    article_patterns = [\n",
        "        re.compile(r\"አንቀጽ\\s*(?:ቁጥር\\s*)?[:\\s]*(\\d+)\", re.IGNORECASE),\n",
        "        re.compile(r\"^[\\s]*(\\d+)[\\.\\)]\\s\", re.MULTILINE),  # Number at start of line\n",
        "        re.compile(r\"አንቀጽ\\s*[\\(\\[]\\s*(\\d+)\\s*[\\)\\]]\", re.IGNORECASE),\n",
        "        re.compile(r\"አንቀጽ\\s*(\\d+)\", re.IGNORECASE),\n",
        "    ]\n",
        "    \n",
        "    # Sub-article patterns\n",
        "    sub_article_patterns = [\n",
        "        re.compile(r\"ንዑስ\\s*አንቀጽ\\s*(?:ቁጥር\\s*)?[:\\s]*(\\d+)\", re.IGNORECASE),\n",
        "        re.compile(r\"ንዑስ\\s*አንቀጽ\\s*(\\d+)\", re.IGNORECASE),\n",
        "    ]\n",
        "    \n",
        "    # Split text into lines for processing\n",
        "    lines = text.split(\"\\n\")\n",
        "    \n",
        "    current_article = None\n",
        "    current_article_num = None\n",
        "    current_article_text = []\n",
        "    current_sub_articles = []\n",
        "    \n",
        "    for i, line in enumerate(lines):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        \n",
        "        # Check if this line starts a new article (try all patterns)\n",
        "        article_match = None\n",
        "        for pattern in article_patterns:\n",
        "            article_match = pattern.search(line)\n",
        "            if article_match:\n",
        "                break\n",
        "        \n",
        "        if article_match:\n",
        "            # Save previous article if exists\n",
        "            if current_article_num is not None and current_article_text:\n",
        "                article_text = \"\\n\".join(current_article_text).strip()\n",
        "                if article_text:\n",
        "                    articles.append({\n",
        "                        \"article_number\": current_article_num,\n",
        "                        \"text\": article_text,\n",
        "                        \"sub_articles\": current_sub_articles.copy(),\n",
        "                        \"start_line\": i - len(current_article_text),\n",
        "                        \"end_line\": i,\n",
        "                    })\n",
        "            \n",
        "            # Start new article\n",
        "            current_article_num = article_match.group(1)\n",
        "            current_article_text = [line]\n",
        "            current_sub_articles = []\n",
        "            continue\n",
        "        \n",
        "        # Check if this line is a sub-article (try all patterns)\n",
        "        sub_article_match = None\n",
        "        for pattern in sub_article_patterns:\n",
        "            sub_article_match = pattern.search(line)\n",
        "            if sub_article_match:\n",
        "                break\n",
        "        \n",
        "        if sub_article_match:\n",
        "            sub_article_num = sub_article_match.group(1)\n",
        "            if sub_article_num not in current_sub_articles:\n",
        "                current_sub_articles.append(sub_article_num)\n",
        "        \n",
        "        # Add line to current article\n",
        "        if current_article_num is not None:\n",
        "            current_article_text.append(line)\n",
        "    \n",
        "    # Save last article\n",
        "    if current_article_num is not None and current_article_text:\n",
        "        article_text = \"\\n\".join(current_article_text).strip()\n",
        "        if article_text:\n",
        "            articles.append({\n",
        "                \"article_number\": current_article_num,\n",
        "                \"text\": article_text,\n",
        "                \"sub_articles\": current_sub_articles.copy(),\n",
        "                \"start_line\": len(lines) - len(current_article_text),\n",
        "                \"end_line\": len(lines),\n",
        "            })\n",
        "    \n",
        "    return articles\n",
        "\n",
        "def detect_legal_function(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Detect the legal function of a text chunk.\n",
        "    Returns: 'obligation', 'prohibition', 'permission', 'condition', 'definition', or 'other'\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "    \n",
        "    # Obligation patterns (must, shall, required)\n",
        "    obligation_patterns = [\n",
        "        r'አለበት',\n",
        "        r'ይገባል',\n",
        "        r'ይገደዳል',\n",
        "        r'አስፈላጊ',\n",
        "        r'ግዴታ',\n",
        "        r'መሆን አለበት',\n",
        "        r'መከበር አለበት',\n",
        "    ]\n",
        "    \n",
        "    # Prohibition patterns (may not, forbidden)\n",
        "    prohibition_patterns = [\n",
        "        r'አይፈቀድም',\n",
        "        r'አይቻልም',\n",
        "        r'አይፈቀድ',\n",
        "        r'ክልከላ',\n",
        "        r'ተከለከለ',\n",
        "        r'አይፈቀድም',\n",
        "    ]\n",
        "    \n",
        "    # Permission patterns (may, allowed)\n",
        "    permission_patterns = [\n",
        "        r'ይፈቀዳል',\n",
        "        r'ይችላል',\n",
        "        r'መሆን ይችላል',\n",
        "        r'ፈቃድ',\n",
        "    ]\n",
        "    \n",
        "    # Condition patterns (if, when, unless)\n",
        "    condition_patterns = [\n",
        "        r'ከ\\.\\.\\.\\s*በስተቀር',\n",
        "        r'ካል\\.\\.\\.\\s*በስተቀር',\n",
        "        r'ቢሆንም',\n",
        "        r'እስከሚ\\.\\.\\.\\s*ድረስ',\n",
        "        r'ከሆነ',\n",
        "        r'በሆነ ሁኔታ',\n",
        "        r'ካልሆነ',\n",
        "    ]\n",
        "    \n",
        "    # Definition patterns (means, refers to, is defined as)\n",
        "    definition_patterns = [\n",
        "        r'ማለት',\n",
        "        r'ነው',\n",
        "        r'የሚለው',\n",
        "        r'የሚመለከት',\n",
        "        r'ትርጉም',\n",
        "        r'ፍቺ',\n",
        "    ]\n",
        "    \n",
        "    # Check patterns in order of specificity\n",
        "    for pattern in obligation_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            return 'obligation'\n",
        "    \n",
        "    for pattern in prohibition_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            return 'prohibition'\n",
        "    \n",
        "    for pattern in condition_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            return 'condition'\n",
        "    \n",
        "    for pattern in permission_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            return 'permission'\n",
        "    \n",
        "    for pattern in definition_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            return 'definition'\n",
        "    \n",
        "    return 'other'\n",
        "\n",
        "def split_into_functional_units(text: str, article_num: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Split article text into functional legal units (obligation, prohibition, etc.).\n",
        "    Returns list of functional chunks with their detected function type.\n",
        "    \"\"\"\n",
        "    functional_chunks = []\n",
        "    \n",
        "    # Split by sentence boundaries (Amharic sentence end marker: ።)\n",
        "    sentences = re.split(r'([።]+)', text)\n",
        "    \n",
        "    # Reconstruct sentences (keep the delimiter with previous sentence)\n",
        "    reconstructed_sentences = []\n",
        "    for i in range(0, len(sentences), 2):\n",
        "        if i < len(sentences):\n",
        "            sentence = sentences[i]\n",
        "            if i + 1 < len(sentences):\n",
        "                sentence += sentences[i + 1]\n",
        "            if sentence.strip():\n",
        "                reconstructed_sentences.append(sentence.strip())\n",
        "    \n",
        "    # Group sentences by legal function\n",
        "    current_function = None\n",
        "    current_chunk = []\n",
        "    \n",
        "    for sentence in reconstructed_sentences:\n",
        "        if not sentence or len(sentence) < 10:\n",
        "            continue\n",
        "        \n",
        "        # Detect function of this sentence\n",
        "        sentence_function = detect_legal_function(sentence)\n",
        "        \n",
        "        # If function changes or chunk is getting large, save current chunk\n",
        "        if current_function and current_function != sentence_function and current_chunk:\n",
        "            chunk_text = \" \".join(current_chunk).strip()\n",
        "            if len(chunk_text) > 20:  # Minimum meaningful chunk size\n",
        "                functional_chunks.append({\n",
        "                    \"function\": current_function,\n",
        "                    \"text\": chunk_text,\n",
        "                    \"article_number\": article_num,\n",
        "                })\n",
        "            current_chunk = []\n",
        "        \n",
        "        current_function = sentence_function\n",
        "        current_chunk.append(sentence)\n",
        "        \n",
        "        # If chunk is getting too large, split it\n",
        "        chunk_text = \" \".join(current_chunk)\n",
        "        if len(chunk_text.encode('utf-8')) > 500_000:  # ~500KB per functional chunk\n",
        "            if current_chunk:\n",
        "                chunk_text = \" \".join(current_chunk[:-1]).strip()\n",
        "                if len(chunk_text) > 20:\n",
        "                    functional_chunks.append({\n",
        "                        \"function\": current_function,\n",
        "                        \"text\": chunk_text,\n",
        "                        \"article_number\": article_num,\n",
        "                    })\n",
        "                current_chunk = [current_chunk[-1]]  # Keep last sentence for next chunk\n",
        "    \n",
        "    # Save final chunk\n",
        "    if current_chunk:\n",
        "        chunk_text = \" \".join(current_chunk).strip()\n",
        "        if len(chunk_text) > 20:\n",
        "            functional_chunks.append({\n",
        "                \"function\": current_function or 'other',\n",
        "                \"text\": chunk_text,\n",
        "                \"article_number\": article_num,\n",
        "            })\n",
        "    \n",
        "    # If no functional chunks created (very short article), return as single chunk\n",
        "    if not functional_chunks:\n",
        "        functional_chunks.append({\n",
        "            \"function\": detect_legal_function(text),\n",
        "            \"text\": text,\n",
        "            \"article_number\": article_num,\n",
        "        })\n",
        "    \n",
        "    return functional_chunks\n",
        "\n",
        "def extract_law_metadata(source_title: str) -> Dict:\n",
        "    \"\"\"Extract metadata about the law from source title.\"\"\"\n",
        "    metadata = {\n",
        "        \"document_title\": source_title,\n",
        "        \"law_name\": source_title,\n",
        "        \"domain\": \"unknown\",\n",
        "        \"year\": None,\n",
        "    }\n",
        "    \n",
        "    # Enhanced domain detection from title\n",
        "    title_lower = source_title.lower()\n",
        "    title_amharic = source_title\n",
        "    \n",
        "    # Criminal law patterns\n",
        "    if any(term in title_lower for term in [\"criminal\", \"ወንጀል\", \"procedure\", \"prosecutor\"]):\n",
        "        metadata[\"domain\"] = \"criminal\"\n",
        "    # Commercial law patterns\n",
        "    elif any(term in title_lower for term in [\"commercial\", \"ንግድ\", \"business\", \"trade\"]):\n",
        "        metadata[\"domain\"] = \"commercial\"\n",
        "    # Civil law patterns\n",
        "    elif any(term in title_lower for term in [\"civil\", \"ነጋሪት\", \"contract\", \"tort\"]):\n",
        "        metadata[\"domain\"] = \"civil\"\n",
        "    # Family law patterns\n",
        "    elif any(term in title_lower for term in [\"family\", \"ቤተሰብ\", \"marriage\", \"divorce\"]):\n",
        "        metadata[\"domain\"] = \"family\"\n",
        "    # Labor law\n",
        "    elif any(term in title_lower for term in [\"labour\", \"labor\", \"employment\", \"worker\"]):\n",
        "        metadata[\"domain\"] = \"labor\"\n",
        "    # Court decisions\n",
        "    elif any(term in title_lower for term in [\"court\", \"decision\", \"cassation\", \"supreme\", \"volume\"]):\n",
        "        metadata[\"domain\"] = \"judicial\"\n",
        "    # Procedure\n",
        "    elif any(term in title_lower for term in [\"procedure\", \"procedural\", \"trial\"]):\n",
        "        metadata[\"domain\"] = \"procedure\"\n",
        "    \n",
        "    # Try to extract year\n",
        "    year_match = re.search(r\"(19|20)\\d{2}\", source_title)\n",
        "    if year_match:\n",
        "        metadata[\"year\"] = int(year_match.group())\n",
        "    \n",
        "    return metadata\n",
        "\n",
        "def build_article_chunks(max_docs: int = None) -> pd.DataFrame:\n",
        "    \"\"\"Build article-level chunks from normalized documents.\"\"\"\n",
        "    extracted_files = sorted(cfg.extracted_dir.glob(\"*.json\"))\n",
        "    \n",
        "    if max_docs is not None:\n",
        "        extracted_files = extracted_files[:max_docs]\n",
        "    \n",
        "    chunk_records = []\n",
        "    \n",
        "    for json_path in tqdm(extracted_files, desc=\"Chunking Articles\", unit=\"doc\"):\n",
        "        try:\n",
        "            payload = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
        "            normalized_text = payload.get(\"normalized_text\", \"\")\n",
        "            source_title = payload.get(\"source_title\", json_path.stem)\n",
        "            source_path = payload.get(\"source_path\", str(json_path))\n",
        "            \n",
        "            if not normalized_text:\n",
        "                continue\n",
        "            \n",
        "            # Detect article structure\n",
        "            articles = detect_article_structure(normalized_text)\n",
        "            \n",
        "            if not articles:\n",
        "                # Fallback: if no articles detected, treat entire document as one chunk\n",
        "                print(f\"[WARN] No articles detected in {source_title}, treating as single chunk\")\n",
        "                articles = [{\n",
        "                    \"article_number\": \"1\",\n",
        "                    \"text\": normalized_text,\n",
        "                    \"sub_articles\": [],\n",
        "                    \"start_line\": 0,\n",
        "                    \"end_line\": len(normalized_text.split(\"\\n\")),\n",
        "                }]\n",
        "            \n",
        "            # Extract law metadata\n",
        "            law_metadata = extract_law_metadata(source_title)\n",
        "            \n",
        "            # Create doc_id\n",
        "            doc_id = uuid.uuid5(uuid.NAMESPACE_URL, f\"{source_path}_{source_title}\").hex\n",
        "            \n",
        "            # Create functional chunks for each article\n",
        "            for article_idx, article in enumerate(articles):\n",
        "                article_text = article[\"text\"]\n",
        "                article_num = article[\"article_number\"]\n",
        "                \n",
        "                # Split article into functional units (obligation, prohibition, etc.)\n",
        "                functional_units = split_into_functional_units(article_text, article_num)\n",
        "                \n",
        "                # Create chunk for each functional unit\n",
        "                for func_idx, func_unit in enumerate(functional_units):\n",
        "                    func_text = func_unit[\"text\"]\n",
        "                    func_type = func_unit[\"function\"]\n",
        "                    \n",
        "                    # Check chunk size and split/truncate if too large\n",
        "                    chunk_size_bytes = len(func_text.encode('utf-8'))\n",
        "                    chunk_size_chars = len(func_text)\n",
        "                    \n",
        "                    if chunk_size_bytes > cfg.max_chunk_size_bytes or chunk_size_chars > cfg.max_chunk_chars:\n",
        "                        # Truncate to safe size (functional chunks should be smaller)\n",
        "                        max_chars = min(cfg.max_chunk_chars, 1_000_000)  # Cap at 1M chars for functional chunks\n",
        "                        func_text = func_text[:max_chars] + \"... [truncated]\"\n",
        "                        print(f\"[WARN] Functional chunk {func_type} in article {article_num} ({source_title}) was truncated\")\n",
        "                    \n",
        "                    # Create unique chunk ID\n",
        "                    if len(functional_units) > 1:\n",
        "                        chunk_id = f\"{doc_id}-article-{article_num}-func{func_idx+1}-{func_type}\"\n",
        "                        display_article_num = f\"{article_num}.{func_idx+1}\"\n",
        "                    else:\n",
        "                        chunk_id = f\"{doc_id}-article-{article_num}-{func_type}\"\n",
        "                        display_article_num = article_num\n",
        "                    \n",
        "                    chunk_records.append({\n",
        "                        \"chunk_id\": chunk_id,\n",
        "                        \"doc_id\": doc_id,\n",
        "                        \"article_number\": display_article_num,\n",
        "                        \"sub_article_numbers\": \",\".join(article[\"sub_articles\"]) if article[\"sub_articles\"] else None,\n",
        "                        \"legal_function\": func_type,  # NEW: functional metadata\n",
        "                        \"text\": func_text,\n",
        "                        \"source_path\": source_path,\n",
        "                        \"source_title\": source_title,\n",
        "                        \"document_title\": law_metadata[\"document_title\"],\n",
        "                        \"law_name\": law_metadata[\"law_name\"],\n",
        "                        \"domain\": law_metadata[\"domain\"],\n",
        "                        \"year\": law_metadata[\"year\"],\n",
        "                        \"created_at\": datetime.utcnow().isoformat(),\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed to chunk {json_path}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    df = pd.DataFrame(chunk_records)\n",
        "    \n",
        "    if df.empty:\n",
        "        print(\"No chunks produced. Check extraction step.\")\n",
        "        return df\n",
        "    \n",
        "    # Save chunk manifest\n",
        "    manifest_path = cfg.chunked_dir / \"chunk_manifest.parquet\"\n",
        "    df.to_parquet(manifest_path, index=False)\n",
        "    print(f\"Saved chunk manifest with {len(df)} chunks → {manifest_path}\")\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\nChunking Statistics:\")\n",
        "    print(f\"  Total chunks: {len(df)}\")\n",
        "    print(f\"  Unique documents: {df['doc_id'].nunique()}\")\n",
        "    print(f\"  Domains: {df['domain'].value_counts().to_dict()}\")\n",
        "    if 'legal_function' in df.columns:\n",
        "        print(f\"  Legal functions: {df['legal_function'].value_counts().to_dict()}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Build chunks\n",
        "chunk_df = build_article_chunks()\n",
        "if not chunk_df.empty:\n",
        "    chunk_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini API configured for embeddings\n",
            "Validating 1228 chunks for size limits...\n",
            "Embedding 1228 valid chunks...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gemini embeddings: 100%|██████████| 13/13 [00:44<00:00,  3.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding dimension: 768\n",
            "Removing existing index to create fresh one\n",
            "Creating new HNSW index\n",
            "Index now holds 1228 vectors; saved to /Users/blank/Documents/Foundation Models Course Projects/rag_pipeline/4_vector_db/faiss_index.bin\n",
            "Metadata has 1228 rows; saved to /Users/blank/Documents/Foundation Models Course Projects/rag_pipeline/4_vector_db/metadata.parquet\n",
            "✓ Index and metadata are in sync (1228 vectors/rows)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Normalize Ge'ez numerals function (needed for embedding)\n",
        "def normalize_geez_numerals(text: str) -> str:\n",
        "    \"\"\"Convert Ge'ez numerals to Arabic numerals.\"\"\"\n",
        "    geez_to_arabic = {\n",
        "        \"፩\": \"1\", \"፪\": \"2\", \"፫\": \"3\", \"፬\": \"4\", \"፭\": \"5\",\n",
        "        \"፮\": \"6\", \"፯\": \"7\", \"፰\": \"8\", \"፱\": \"9\", \"፲\": \"10\",\n",
        "    }\n",
        "    normalized = text\n",
        "    for geez, arabic in geez_to_arabic.items():\n",
        "        normalized = normalized.replace(geez, arabic)\n",
        "    return normalized\n",
        "\n",
        "# Configure Gemini API\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    GEMINI_AVAILABLE = True\n",
        "    if cfg.gemini_api_key:\n",
        "        genai.configure(api_key=cfg.gemini_api_key)\n",
        "        print(\"Gemini API configured for embeddings\")\n",
        "    else:\n",
        "        print(\"[WARN] Gemini API key not found\")\n",
        "        GEMINI_AVAILABLE = False\n",
        "except ImportError:\n",
        "    print(\"[WARN] google-generativeai not installed\")\n",
        "    GEMINI_AVAILABLE = False\n",
        "\n",
        "def embed_with_gemini(texts: List[str], task_type: str = \"RETRIEVAL_DOCUMENT\") -> np.ndarray:\n",
        "    \"\"\"Generate embeddings using Gemini API with batching and rate limiting.\"\"\"\n",
        "    if not GEMINI_AVAILABLE:\n",
        "        raise ValueError(\"Gemini not available\")\n",
        "    \n",
        "    embeddings = []\n",
        "    batch_size = cfg.embedding_batch_size\n",
        "    \n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Gemini embeddings\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        try:\n",
        "            result = genai.embed_content(\n",
        "                model=cfg.gemini_embedding_model,\n",
        "                content=batch,\n",
        "                task_type=task_type\n",
        "            )\n",
        "            batch_embeddings = result['embedding']\n",
        "            embeddings.extend(batch_embeddings)\n",
        "            \n",
        "            # Rate limiting: Free tier limit is 100 requests/minute\n",
        "            if i + batch_size < len(texts):\n",
        "                time.sleep(0.7)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Batch {i} failed: {e}\")\n",
        "            # Retry once after delay\n",
        "            time.sleep(2)\n",
        "            try:\n",
        "                result = genai.embed_content(\n",
        "                    model=cfg.gemini_embedding_model,\n",
        "                    content=batch,\n",
        "                    task_type=task_type\n",
        "            )\n",
        "                batch_embeddings = result['embedding']\n",
        "                embeddings.extend(batch_embeddings)\n",
        "            except Exception as e2:\n",
        "                print(f\"[ERROR] Retry failed for batch {i}: {e2}\")\n",
        "                # Fallback to zeros (will cause issues, but allows continuation)\n",
        "                embeddings.extend([np.zeros(768) for _ in batch])\n",
        "    \n",
        "    return np.array(embeddings, dtype=np.float32)\n",
        "\n",
        "def build_faiss_index(df: pd.DataFrame) -> faiss.Index:\n",
        "    \"\"\"\n",
        "    Build FAISS index from chunks with size validation.\n",
        "    Ensures numerals are normalized before embedding to improve semantic similarity.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No chunks to embed\")\n",
        "    \n",
        "    # Filter and validate chunks before embedding\n",
        "    print(f\"Validating {len(df)} chunks for size limits...\")\n",
        "    \n",
        "    valid_chunks = []\n",
        "    valid_indices = []\n",
        "    skipped_count = 0\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        text = row[\"text\"]\n",
        "        \n",
        "        # Ensure numerals are normalized before embedding (critical for semantic similarity)\n",
        "        # This should already be done, but double-check to avoid numeral-driven similarity\n",
        "        text = normalize_geez_numerals(text)\n",
        "        \n",
        "        chunk_size_bytes = len(text.encode('utf-8'))\n",
        "        chunk_size_chars = len(text)\n",
        "        \n",
        "        # Skip chunks that are too large (shouldn't happen after chunking fix, but double-check)\n",
        "        if chunk_size_bytes > cfg.max_chunk_size_bytes:\n",
        "            print(f\"[WARN] Skipping chunk {row.get('chunk_id', idx)}: too large ({chunk_size_bytes:,} bytes)\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "        \n",
        "        # Truncate if slightly over character limit\n",
        "        if chunk_size_chars > cfg.max_chunk_chars:\n",
        "            text = text[:cfg.max_chunk_chars] + \"... [truncated]\"\n",
        "            print(f\"[WARN] Truncating chunk {row.get('chunk_id', idx)}: {chunk_size_chars:,} chars\")\n",
        "        \n",
        "        valid_chunks.append(text)\n",
        "        valid_indices.append(idx)\n",
        "    \n",
        "    if skipped_count > 0:\n",
        "        print(f\"Skipped {skipped_count} chunks that were too large\")\n",
        "    \n",
        "    if not valid_chunks:\n",
        "        raise ValueError(\"No valid chunks to embed after size validation\")\n",
        "    \n",
        "    print(f\"Embedding {len(valid_chunks)} valid chunks...\")\n",
        "    \n",
        "    if GEMINI_AVAILABLE:\n",
        "        embeddings = embed_with_gemini(valid_chunks, task_type=\"RETRIEVAL_DOCUMENT\")\n",
        "    else:\n",
        "        raise ValueError(\"Gemini embeddings required but not available\")\n",
        "    \n",
        "    # Filter dataframe to only include valid chunks\n",
        "    df_valid = df.loc[valid_indices].copy()\n",
        "    \n",
        "    dimension = embeddings.shape[1]\n",
        "    print(f\"Embedding dimension: {dimension}\")\n",
        "    \n",
        "    # Create FAISS index\n",
        "    if cfg.index_path.exists():\n",
        "        print(f\"Removing existing index to create fresh one\")\n",
        "        cfg.index_path.unlink()\n",
        "    \n",
        "    print(\"Creating new HNSW index\")\n",
        "    index = faiss.IndexHNSWFlat(dimension, 32)\n",
        "    index.hnsw.efConstruction = 200\n",
        "    index.hnsw.efSearch = 64\n",
        "    index.add(embeddings.astype(\"float32\"))\n",
        "    \n",
        "    # Save metadata (only valid chunks)\n",
        "    df_valid.to_parquet(cfg.metadata_path, index=False)\n",
        "    \n",
        "    # Save index\n",
        "    faiss.write_index(index, str(cfg.index_path))\n",
        "    \n",
        "    print(f\"Index now holds {index.ntotal} vectors; saved to {cfg.index_path}\")\n",
        "    print(f\"Metadata has {len(df_valid)} rows; saved to {cfg.metadata_path}\")\n",
        "    \n",
        "    # Verify they match\n",
        "    if index.ntotal != len(df_valid):\n",
        "        print(f\"[WARN] Mismatch: index has {index.ntotal} vectors but metadata has {len(df_valid)} rows\")\n",
        "    else:\n",
        "        print(f\"✓ Index and metadata are in sync ({index.ntotal} vectors/rows)\")\n",
        "    \n",
        "    return index\n",
        "\n",
        "# Build index if chunks exist\n",
        "if not chunk_df.empty:\n",
        "    faiss_index = build_faiss_index(chunk_df)\n",
        "else:\n",
        "    print(\"No chunks to embed. Run chunking step first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing retrieval with query: የንግድ ስምምነት መፈጸም\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gemini embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Retrieved 2 chunks:\n",
            "\n",
            "Rank 1 (score: 0.7006):\n",
            "  Law: Ethiopia-Commercial-Code-Amharic-Proclamation-No.-1243_2021\n",
            "  Article: 1.734\n",
            "  Domain: commercial\n",
            "  Function: other\n",
            "  Text preview: ገንዘብ ጠያቂውም በክርክር ሂዯቱ ተሳታፉ ሇመሆን አይችሌም።...\n",
            "\n",
            "Rank 2 (score: 0.7009):\n",
            "  Law: 1374\n",
            "  Article: 9.2\n",
            "  Domain: unknown\n",
            "  Function: other\n",
            "  Text preview: ቀጥታ ስርጭትን በሚመለከት ዝርዝሩ ቦርዱ በሚያወጣው መመሪያ ይወሰናል።...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "# Normalize Ge'ez numerals function (needed for retrieval)\n",
        "def normalize_geez_numerals(text: str) -> str:\n",
        "    \"\"\"Convert Ge'ez numerals to Arabic numerals.\"\"\"\n",
        "    geez_to_arabic = {\n",
        "        \"፩\": \"1\", \"፪\": \"2\", \"፫\": \"3\", \"፬\": \"4\", \"፭\": \"5\",\n",
        "        \"፮\": \"6\", \"፯\": \"7\", \"፰\": \"8\", \"፱\": \"9\", \"፲\": \"10\",\n",
        "    }\n",
        "    normalized = text\n",
        "    for geez, arabic in geez_to_arabic.items():\n",
        "        normalized = normalized.replace(geez, arabic)\n",
        "    return normalized\n",
        "\n",
        "def load_index_and_metadata() -> Tuple[faiss.Index, pd.DataFrame]:\n",
        "    \"\"\"Load FAISS index and metadata.\"\"\"\n",
        "    if not cfg.index_path.exists():\n",
        "        raise FileNotFoundError(\"FAISS index not found. Run embedding step first.\")\n",
        "    \n",
        "    index = faiss.read_index(str(cfg.index_path))\n",
        "    \n",
        "    if not cfg.metadata_path.exists():\n",
        "        raise FileNotFoundError(f\"Metadata file missing: {cfg.metadata_path}\")\n",
        "    \n",
        "    df = pd.read_parquet(cfg.metadata_path)\n",
        "    \n",
        "    return index, df\n",
        "\n",
        "def retrieve_legal_context(\n",
        "    query: str,\n",
        "    top_k: int = None,\n",
        "    domain_filter: Optional[str] = None,\n",
        "    function_filter: Optional[str] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Retrieve relevant legal articles for a query.\n",
        "    \n",
        "    For sentence simplification: retrieve 2 chunks (hard limit).\n",
        "    More context does not improve simplification quality and can cause:\n",
        "    - Copying legal language instead of simplifying\n",
        "    - Losing aggressive simplification\n",
        "    - Cognitive overload for the generator\n",
        "    \n",
        "    Args:\n",
        "        query: Legal sentence or query text (in Amharic)\n",
        "        top_k: Number of chunks to retrieve (default: cfg.top_k_retrieval, hard cap at 2 for simplification)\n",
        "        domain_filter: Optional domain filter (\"criminal\", \"commercial\", \"civil\", \"family\")\n",
        "        function_filter: Optional legal function filter (\"obligation\", \"prohibition\", \"permission\", \"condition\", \"definition\")\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with retrieved chunks and metadata\n",
        "    \"\"\"\n",
        "    if top_k is None:\n",
        "        top_k = cfg.top_k_retrieval\n",
        "    \n",
        "    # Hard cap at 2 for simplification (max 3 only if sentence has explicit conditions+exceptions)\n",
        "    # This prevents context overload and ensures cleaner, more consistent simplification\n",
        "    if top_k > 2:\n",
        "        top_k = 2\n",
        "    \n",
        "    index, df = load_index_and_metadata()\n",
        "    \n",
        "    # Verify index and metadata are in sync\n",
        "    index_size = index.ntotal\n",
        "    metadata_size = len(df)\n",
        "    if index_size != metadata_size:\n",
        "        print(f\"[WARN] Index size ({index_size}) doesn't match metadata size ({metadata_size})\")\n",
        "        max_valid_idx = min(index_size, metadata_size) - 1\n",
        "    else:\n",
        "        max_valid_idx = metadata_size - 1\n",
        "    \n",
        "    # Normalize query numerals before embedding (critical for semantic similarity)\n",
        "    query_normalized = normalize_geez_numerals(query)\n",
        "    \n",
        "    # Generate query embedding\n",
        "    if GEMINI_AVAILABLE:\n",
        "        query_emb = embed_with_gemini([query_normalized], task_type=\"RETRIEVAL_QUERY\")[0]\n",
        "    else:\n",
        "        raise ValueError(\"Gemini embeddings required but not available\")\n",
        "    \n",
        "    query_emb = query_emb.astype(\"float32\").reshape(1, -1)\n",
        "    \n",
        "    # Search (retrieve more if filtering to ensure we get enough after filtering)\n",
        "    search_k = top_k * 5 if (domain_filter or function_filter) else top_k\n",
        "    distances, indices = index.search(query_emb, search_k)\n",
        "    \n",
        "    hits = []\n",
        "    for rank, (score, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "        if idx == -1 or idx > max_valid_idx or idx < 0:\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            row = df.iloc[idx].to_dict()\n",
        "            \n",
        "            # Apply domain filter if specified\n",
        "            if domain_filter and row.get(\"domain\") != domain_filter:\n",
        "                continue\n",
        "            \n",
        "            # Apply function filter if specified\n",
        "            if function_filter and row.get(\"legal_function\") != function_filter:\n",
        "                continue\n",
        "            \n",
        "            row.update({\n",
        "                \"rank\": len(hits) + 1,\n",
        "                \"score\": float(score),\n",
        "            })\n",
        "            hits.append(row)\n",
        "            \n",
        "            if len(hits) >= top_k:\n",
        "                break\n",
        "        except IndexError as e:\n",
        "            print(f\"[WARN] Failed to access row {idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return pd.DataFrame(hits)\n",
        "\n",
        "# Test retrieval\n",
        "if cfg.index_path.exists():\n",
        "    test_query = \"የንግድ ስምምነት መፈጸም\"\n",
        "    print(f\"Testing retrieval with query: {test_query}\")\n",
        "    results = retrieve_legal_context(test_query, top_k=2)  # Hard limit: 2 chunks for simplification\n",
        "    if not results.empty:\n",
        "        print(f\"\\nRetrieved {len(results)} chunks:\")\n",
        "        for _, row in results.iterrows():\n",
        "            print(f\"\\nRank {row['rank']} (score: {row['score']:.4f}):\")\n",
        "            print(f\"  Law: {row['law_name']}\")\n",
        "            print(f\"  Article: {row['article_number']}\")\n",
        "            print(f\"  Domain: {row['domain']}\")\n",
        "            if 'legal_function' in row:\n",
        "                print(f\"  Function: {row['legal_function']}\")\n",
        "            print(f\"  Text preview: {row['text'][:200]}...\")\n",
        "    else:\n",
        "        print(\"No results found.\")\n",
        "else:\n",
        "    print(\"Index not found. Run embedding step first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Usage Example\n",
        "\n",
        "Example of how to use the RAG system for legal text simplification context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example: Retrieving context for legal sentence simplification\n",
            "\n",
            "Original legal sentence:\n",
            "በዚህ አዋጅ መሠረት የተወሰነው ውሳኔ በሁሉም አካላት መከበር አለበት።\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gemini embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Formatted context for model:\n",
            "================================================================================\n",
            "Legal sentence to simplify: በዚህ አዋጅ መሠረት የተወሰነው ውሳኔ በሁሉም አካላት መከበር አለበት።\n",
            "\n",
            "\n",
            "Relevant legal context:\n",
            "\n",
            "---\n",
            "Law: 6cf83-23-e18ba8e18d8c.e18ca0.e18d8d.e189a4e189b5-e188a0e189a0e188ad-e188b0e1889a-e189bde1888ee189b5-e18b8de188b3e18a94e18b8ee189bd\n",
            "Article: 89.17\n",
            "Domain: unknown\n",
            "Function: definition\n",
            "\n",
            "\n",
            "---\n",
            "Law: Ethiopia-Commercial-Code-Amharic-Proclamation-No.-1243_2021\n",
            "Article: 1.780\n",
            "Domain: commercial\n",
            "Function: other\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "This formatted context would be passed to your fine-tuned model.\n",
            "The model uses this context to understand legal meaning before simplifying.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def format_retrieved_context(query: str, retrieved_chunks: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Format retrieved chunks as context for the simplification model.\n",
        "    This is what you would pass to your fine-tuned AfriByT5 model.\n",
        "    \"\"\"\n",
        "    if retrieved_chunks.empty:\n",
        "        return f\"Legal sentence to simplify: {query}\\n\\nNo relevant legal context found.\"\n",
        "    \n",
        "    context_parts = [f\"Legal sentence to simplify: {query}\", \"\\n\\nRelevant legal context:\"]\n",
        "    \n",
        "    for _, row in retrieved_chunks.iterrows():\n",
        "        function_info = f\"Function: {row.get('legal_function', 'unknown')}\" if 'legal_function' in row else \"\"\n",
        "        context_parts.append(\n",
        "            f\"\\n---\\n\"\n",
        "            f\"Law: {row['law_name']}\\n\"\n",
        "            f\"Article: {row['article_number']}\\n\"\n",
        "            f\"Domain: {row['domain']}\\n\"\n",
        "            f\"{function_info}\\n\" if function_info else \"\"\n",
        "            f\"Text: {row['text']}\"\n",
        "        )\n",
        "    \n",
        "    return \"\\n\".join(context_parts)\n",
        "\n",
        "# Example usage\n",
        "example_legal_sentence = \"በዚህ አዋጅ መሠረት የተወሰነው ውሳኔ በሁሉም አካላት መከበር አለበት።\"\n",
        "\n",
        "print(\"Example: Retrieving context for legal sentence simplification\")\n",
        "print(f\"\\nOriginal legal sentence:\\n{example_legal_sentence}\")\n",
        "\n",
        "if cfg.index_path.exists():\n",
        "    retrieved = retrieve_legal_context(example_legal_sentence, top_k=2)  # Hard limit: 2 chunks for simplification\n",
        "    \n",
        "    if not retrieved.empty:\n",
        "        formatted_context = format_retrieved_context(example_legal_sentence, retrieved)\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Formatted context for model:\")\n",
        "        print(\"=\"*80)\n",
        "        print(formatted_context)\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"\\nThis formatted context would be passed to your fine-tuned model.\")\n",
        "        print(\"The model uses this context to understand legal meaning before simplifying.\")\n",
        "    else:\n",
        "        print(\"\\nNo relevant context retrieved.\")\n",
        "else:\n",
        "    print(\"\\nIndex not found. Run embedding step first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Session Recovery & Status\n",
        "\n",
        "Check processing status and resume from where you left off.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG Pipeline Status:\n",
            "==================================================\n",
            "pdfs_found: 94\n",
            "pdfs_extracted: 94\n",
            "pdfs_normalized: 94\n",
            "chunks_created: 1228\n",
            "index_exists: True\n",
            "metadata_exists: True\n",
            "unique_documents: 65\n",
            "domains: {'commercial': 793, 'judicial': 324, 'unknown': 80, 'criminal': 15, 'labor': 14, 'procedure': 2}\n",
            "index_vectors: 1228\n",
            "==================================================\n",
            "\n",
            "✓ RAG system is ready for retrieval!\n"
          ]
        }
      ],
      "source": [
        "def check_processing_status() -> Dict:\n",
        "    \"\"\"Check the status of the RAG pipeline.\"\"\"\n",
        "    status = {\n",
        "        \"pdfs_found\": len(list(cfg.raw_pdf_dir.glob(\"*.pdf\"))),\n",
        "        \"pdfs_extracted\": len(list(cfg.extracted_dir.glob(\"*.json\"))),\n",
        "        \"pdfs_normalized\": len(list(cfg.normalized_dir.glob(\"*.txt\"))),\n",
        "        \"chunks_created\": 0,\n",
        "        \"index_exists\": cfg.index_path.exists(),\n",
        "        \"metadata_exists\": cfg.metadata_path.exists(),\n",
        "    }\n",
        "    \n",
        "    if cfg.metadata_path.exists():\n",
        "        try:\n",
        "            df = pd.read_parquet(cfg.metadata_path)\n",
        "            status[\"chunks_created\"] = len(df)\n",
        "            status[\"unique_documents\"] = df[\"doc_id\"].nunique()\n",
        "            status[\"domains\"] = df[\"domain\"].value_counts().to_dict()\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    if cfg.index_path.exists():\n",
        "        try:\n",
        "            index = faiss.read_index(str(cfg.index_path))\n",
        "            status[\"index_vectors\"] = index.ntotal\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    return status\n",
        "\n",
        "status = check_processing_status()\n",
        "print(\"RAG Pipeline Status:\")\n",
        "print(\"=\"*50)\n",
        "for key, value in status.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if status[\"pdfs_extracted\"] < status[\"pdfs_found\"]:\n",
        "    print(f\"\\n⚠️  {status['pdfs_found'] - status['pdfs_extracted']} PDFs still need extraction\")\n",
        "    print(\"   Run: extract_and_normalize_pdfs(max_docs=None)\")\n",
        "\n",
        "if status[\"chunks_created\"] == 0 and status[\"pdfs_extracted\"] > 0:\n",
        "    print(f\"\\n⚠️  Chunking needed for {status['pdfs_extracted']} extracted documents\")\n",
        "    print(\"   Run: build_article_chunks()\")\n",
        "\n",
        "if not status[\"index_exists\"] and status[\"chunks_created\"] > 0:\n",
        "    print(f\"\\n⚠️  Indexing needed for {status['chunks_created']} chunks\")\n",
        "    print(\"   Run: build_faiss_index(chunk_df)\")\n",
        "\n",
        "if status[\"index_exists\"] and status[\"chunks_created\"] > 0:\n",
        "    print(\"\\n✓ RAG system is ready for retrieval!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
